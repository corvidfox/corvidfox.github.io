[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome! Here you’ll find a small collection of personal projects that reflect some of my skills, interests, and growth. Feel free to explore — use the filters to browse by category and dive into the areas that interest you the most.\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)\n\n\n\nSQL\n\nR\n\nPython\n\nDashboard\n\nData Analysis\n\nBusiness Intelligence\n\nExploratory Analysis\n\nRetail Analytics\n\nChinook\n\nShort\n\nDuckDB\n\n\n\nThis project explores the Chinook dataset — a mock digital music store — to uncover key business insights around revenue, customers, and performance. It combines SQL analysis with dashboard development to present findings visually.\n\n\n\n\n\nJun 30, 2025\n\n\nMorrigan M.\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m mainly excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I was a critical care nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner.\n\n\n\n\nMPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology\n\n\n\n\n\nGraduate Research Assistant, February 2023 - Present\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\nCritical Care Travel Nurse, March 2021 - December 2022\nTravel Nurse Across America (USA)\n\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\nRegistered Nurse (Residency), March 2020 - March 2021\nRochester General Hospital (Rochester, NY, USA)\nPublic Safety Communications Specialist, February 2018 - February 2020\nCity of Plano (Plano, TX, USA)"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m mainly excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I was a critical care nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Morri",
    "section": "",
    "text": "MPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Morri",
    "section": "",
    "text": "Graduate Research Assistant, February 2023 - Present\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\nCritical Care Travel Nurse, March 2021 - December 2022\nTravel Nurse Across America (USA)\n\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\nRegistered Nurse (Residency), March 2020 - March 2021\nRochester General Hospital (Rochester, NY, USA)\nPublic Safety Communications Specialist, February 2018 - February 2020\nCity of Plano (Plano, TX, USA)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m especially excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I spent over a decade as a nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner. (Also: the dog’s name is Lucy. She does not code.)\n\n\n\n\nMPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology\n\n\n\n\n\n\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\n\n\n\nTravel Nurse Across America, USA\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\n\n\n\nRochester General Hospital, Rochester, NY, USA\n\n\n\nCity of Plano, Plano, TX, USA"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m especially excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I spent over a decade as a nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner. (Also: the dog’s name is Lucy. She does not code.)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Morri",
    "section": "",
    "text": "MPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About Morri",
    "section": "",
    "text": "UTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\n\n\n\nTravel Nurse Across America, USA\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\n\n\n\nRochester General Hospital, Rochester, NY, USA\n\n\n\nCity of Plano, Plano, TX, USA"
  },
  {
    "objectID": "projects/draft.html",
    "href": "projects/draft.html",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "Goal: Build a BI dashboard for a fictional digital media business.\nFocus: Revenue insights, customer analysis, business performance.\nWhy it matters: Business stakeholders need quick, clear access to KPIs.\n\n\n\nThe following tools were used to merge, analyze, and present the data:\n\n\n\nTool\nPurpose\n\n\n\n\nSQL\nData transformation and KPIs\n\n\nDuckDB\nLightweight, embedded relational database\n\n\nR + Shiny\nDashboard development (R-based)\n\n\nPython + Dash\nDashboard development (Python-based)\n\n\n\nThe project demonstrates dashboard development using both R (Shiny) and Python (Dash) to showcase flexibility across ecosystems.\n[NOTE TO SELF - DELETE ONCE REVISED: This is a work in progress. I plan to make a dashboard with R and one with Python, just to show I can do both. I will update this to reflect the final project once it’s done.]\n\nRPython\n\n\n\n\nShow Code\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(DBI)\nlibrary(plotly)\nlibrary(countrycode)\n\n\n\n\n\n\nShow Code\nimport duckdb\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pycountry\n\n\n\n\n\n\n\n\nThis project is designed to help answer the following business questions:\n\nWhere is revenue coming from geographically?\nWhat genres or artists generate the most income?\nHow many customers are repeat buyers?\nHow do sales trends evolve over time?"
  },
  {
    "objectID": "projects/draft.html#technology-stack",
    "href": "projects/draft.html#technology-stack",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "The following tools were used to merge, analyze, and present the data:\n\n\n\nTool\nPurpose\n\n\n\n\nSQL\nData transformation and KPIs\n\n\nDuckDB\nLightweight, embedded relational database\n\n\nR + Shiny\nDashboard development (R-based)\n\n\nPython + Dash\nDashboard development (Python-based)\n\n\n\nThe project demonstrates dashboard development using both R (Shiny) and Python (Dash) to showcase flexibility across ecosystems.\n[NOTE TO SELF - DELETE ONCE REVISED: This is a work in progress. I plan to make a dashboard with R and one with Python, just to show I can do both. I will update this to reflect the final project once it’s done.]\n\nRPython\n\n\n\n\nShow Code\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(DBI)\nlibrary(plotly)\nlibrary(countrycode)\n\n\n\n\n\n\nShow Code\nimport duckdb\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pycountry"
  },
  {
    "objectID": "projects/draft.html#business-questions",
    "href": "projects/draft.html#business-questions",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "This project is designed to help answer the following business questions:\n\nWhere is revenue coming from geographically?\nWhat genres or artists generate the most income?\nHow many customers are repeat buyers?\nHow do sales trends evolve over time?"
  },
  {
    "objectID": "projects/draft.html#connection-and-validation",
    "href": "projects/draft.html#connection-and-validation",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Connection and Validation",
    "text": "Connection and Validation\nA connection was made to the DuckDB database file.\n\nRPython\n\n\n\n\nShow Code\ncon_chinook &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"../data/chinook.duckdb\",\n  read_only = TRUE\n  )\n\n\n\n\n\n\nShow Code\ncon_chinook = duckdb.connect(\"../data/chinook.duckdb\", read_only = True)\n\n\n\n\n\nInitial exploration aimed to verify the expected data range and structure described in the documentation.\nA query of date values in the InvoiceDate table confirmed that the data contained records with a date range from 2009-01-01 to 2013-12-22.\n\n\n\nShow Code\n\nSQL\n\n-- Get date range of Invoices\nSELECT \n  MIN(i.InvoiceDate) as MinDate, \n  MAX(i.InvoiceDate) as MaxDate\nFROM Invoice i;\n\n\n\n\n1 records\n\n\nMinDate\nMaxDate\n\n\n\n\n2009-01-01\n2013-12-22\n\n\n\n\n\nAs expected, InvoiceLine and Track had the highest number of unique records, reflecting their one-to-many relationships with Invoice and Album, respectively. Metadata tables such as Genre and MediaType had fewer unique values.\n\n\n\nShow Code\n\nSQL\n\n-- Get Number of Unique Key Values in Each Table\nSELECT \n  'Employees' AS TableName, \n  COUNT(DISTINCT EmployeeId) AS UniqueKeys \nFROM Employee\nUNION ALL\nSELECT \n  'Customers' AS TableName, \n  COUNT(DISTINCT Customerid) AS UniqueKeys \nFROM Customer\nUNION ALL\nSELECT \n  'Invoices' AS TableName, \n  COUNT(DISTINCT InvoiceId) AS UniqueKeys \nFROM Invoice\nUNION ALL\nSELECT \n  'Invoice Lines' AS TableName, \n  COUNT(DISTINCT InvoiceLineId) AS UniqueKeys \nFROM InvoiceLine\nUNION ALL\nSELECT \n  'Tracks' AS TableName, \n  COUNT(DISTINCT TrackId) AS UniqueKeys \nFROM Track\nUNION ALL\nSELECT \n  'Artists' AS TableName, \n  COUNT(DISTINCT ArtistId) AS UniqueKeys \nFROM Artist\nUNION ALL\nSELECT \n  'Albums' AS TableName, \n  COUNT(DISTINCT AlbumId) AS UniqueKeys \nFROM Album\nUNION ALL\nSELECT \n  'Genres' AS TableName, \n  COUNT(DISTINCT GenreId) AS UniqueKeys \nFROM Genre\nUNION ALL\nSELECT \n  'Media Types' AS TableName,\n  COUNT(DISTINCT MediaTypeId) AS UniqueKeys \nFROM MediaType\nUNION ALL\nSELECT \n  'Playlists' AS TableName, \n  COUNT(DISTINCT PlaylistId) AS UniqueKeys \nFROM Playlist\nORDER BY UniqueKeys DESC;\n\n\n\n\nDisplaying records 1 - 10\n\n\nTableName\nUniqueKeys\n\n\n\n\nTracks\n3503\n\n\nInvoice Lines\n2240\n\n\nInvoices\n412\n\n\nAlbums\n347\n\n\nArtists\n275\n\n\nCustomers\n59\n\n\nGenres\n25\n\n\nPlaylists\n18\n\n\nEmployees\n8\n\n\nMedia Types\n5\n\n\n\n\n\nWith structure confirmed, attention was turned towards the identified business questions."
  },
  {
    "objectID": "projects/draft.html#exploratory-visualizations",
    "href": "projects/draft.html#exploratory-visualizations",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Exploratory Visualizations",
    "text": "Exploratory Visualizations\nIn preparation for exploratory visualization generation, the data is retrieved using SQL queries and prepared in both R and Python.\n\nRPython\n\n\n\n\nShow Code\n# SQL Queries\n## Yearly Breakdown\nres_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n    -- Get Country and Year for grouping\n    i.BillingCountry as Country, \n    YEAR(i.InvoiceDate) as Year,\n    -- Calculate Total Revenue\n    SUM(i.Total) AS TotalRevenue,\n    -- Calculate % of Total/Global Revenue\n    ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n    -- Get Number of Customers\n    COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n    -- Calculate Revenue per Customer\n    ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\nFROM Customer c\nJOIN Invoice i on c.CustomerId == i.CustomerId\nGROUP BY i.BillingCountry, Year\n-- Sort Revenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n## Total (all years)\nres_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n    -- Get Country for grouping\n    i.BillingCountry as Country,\n    -- Set 'Year' to 'All' for grouping\n    'All' AS Year,\n    -- Calculate Total Revenue\n    SUM(i.Total) AS TotalRevenue,\n    -- Calculate % of Total/Global Revenue\n    ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n    -- Get Number of Customers\n    COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n    -- Calculate Revenue per Customer\n    ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\nFROM Customer c\nJOIN Invoice i on c.CustomerId == i.CustomerId\nGROUP BY i.BillingCountry,\n-- Sort Revenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n# Combine data frames in R\nres_df &lt;- dplyr::bind_rows(\n  res_agg_df,\n  res_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  ) |&gt;\n  dplyr::mutate(\n    ## Add ISO Country Codes\n    iso_alpha = countrycode::countrycode(\n      Country, \n      origin = 'country.name', \n      destination = 'iso3c'\n      ),\n    ## Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $TotalRevenue.##\")\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(TotalRevenue, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get vector of unique years (layers/traces) - order with \"All\" first.\nyears &lt;- c(\"All\", sort(unique(res_yearly_df$Year)))\n\n\n\n\n\n\nShow Code\n# SQL Queries\n## Yearly Breakdown\nres_yearly_df = con_chinook.execute(\n    \"\"\"SELECT \n      -- Get Country and Year for grouping\n      i.BillingCountry as Country, \n      YEAR(i.InvoiceDate) as Year,\n      -- Calculate Total Revenue\n      SUM(i.Total) AS TotalRevenue,\n      -- Calculate % of Total/Global Revenue\n      ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n      -- Get Number of Customers\n      COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n      -- Calculate Revenue per Customer\n      ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\n  FROM Customer c\n  JOIN Invoice i on c.CustomerId == i.CustomerId\n  GROUP BY i.BillingCountry, Year\n  -- Sort Revenue (Highest to Lowest)\n  ORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n\n## Total (all years)\nres_agg_df = con_chinook.execute(\n    \"\"\"SELECT \n      -- Get Country for grouping\n      i.BillingCountry as Country,\n      -- Set 'Year' to 'All' for grouping\n      'All' AS Year,\n      -- Calculate Total Revenue\n      SUM(i.Total) AS TotalRevenue,\n      -- Calculate % of Total/Global Revenue\n      ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n      -- Get Number of Customers\n      COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n      -- Calculate Revenue per Customer\n      ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\n  FROM Customer c\n  JOIN Invoice i on c.CustomerId == i.CustomerId\n  GROUP BY i.BillingCountry,\n  -- Sort Revenue (Highest to Lowest)\n  ORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n\n# Combine data frames and ensure consistent types\nres_df = pd.concat([\n  res_agg_df,\n  res_yearly_df.assign(Year=res_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n\n# Add ISO Country Codes\ndef get_iso_alpha3(country_name):\n try:\n   return pycountry.countries.lookup(country_name).alpha_3\n except LookupError:\n   return None\n \nres_df['iso_alpha'] = res_df['Country'].apply(get_iso_alpha3)\n\n# Get unique years (layers/traces) - order with \"All\" first.\nyears = [\"All\"] + sorted(res_df[res_df['Year'] != 'All']['Year'].unique().tolist())\n\n\n\n\n\n\nTotal Revenue by Country\nTotal revenue per country across the entire data set was visualized with a Choropeth plot.\nFor best viewing experience, browser is recommended - optimization of these exploratory visualizations is limited.\n\nRPython\n\n\n\n\nShow Code\n# Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $TotalRevenue.##\")\nres_df &lt;- res_df |&gt; \n  dplyr::mutate(\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(TotalRevenue, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get minimum and maximum values for TotalRevenue (Colorbar consistency)\nz_min_val &lt;- min(res_df$TotalRevenue, na.rm = TRUE)\nz_max_val &lt;- max(res_df$TotalRevenue, na.rm = TRUE)\n\n# Generate plotly Choropleth\nfig &lt;- plotly::plot_ly(\n  data = res_df,\n  type = 'choropleth',\n  locations = ~iso_alpha,\n  z = ~TotalRevenue,\n  # Set hover text to only display our desired, formatted output\n  text = ~hover_text,\n  hoverinfo = \"text\",\n  frame = ~Year,\n  # Set minimum and maximum TotalRevenue values, for consistent scale\n  zmin = z_min_val,\n  zmax = z_max_val,\n  # Title Colorbar/Legend\n  colorbar = list(\n    title = \"Total Revenue (USD$)\"\n  ),\n  # Color-blind friendly color scale\n  colorscale = \"Viridis\",\n  reversescale = TRUE,\n  showscale = TRUE,\n  # Give national boundaries a dark gray outline\n  marker = list(line = list(color = \"darkgrey\", width = 0.5))\n)\n\n# Layout with animation controls\nfig &lt;- fig %&gt;%\n  plotly::layout(\n    title = list(\n      text = \"Total Revenue by Country &lt;br&gt; 2009-01-01 to 2013-12-22\",\n      x = 0.5,\n      xanchor = \"center\",\n      font = list(size = 18)\n    ),\n    geo = list(\n      # Add a neat little frame around the world\n      showframe = TRUE, \n      # Add coast lines - ensures countries that aren't in data are seen\n      showcoastlines = TRUE, \n      # Use natural earth projection\n      projection = list(type = 'natural earth')\n      ),\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        showactive = TRUE,\n        buttons = purrr::map(years, function(yr) {\n          list(\n            method = \"animate\",\n            args = list(list(yr), list(mode = \"immediate\", frame = list(duration = 0, redraw = TRUE))),\n            label = yr\n          )\n        }),\n        # Positioning of dropdown menu\n        x = 0.1,\n        y = 1.15,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    ),\n    margin = list(t = 80)\n  ) %&gt;%\n  plotly::animation_opts(frame = 1000, transition = 0, redraw = TRUE)\n\n# Display interactive plot\nfig\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Specify hover text\nres_df['hover_text'] = res_df.apply( \\\n  lambda row: f\"&lt;b&gt;{row['Country']}&lt;/b&gt;&lt;br&gt; ${row['TotalRevenue']:.2f}\", axis = 1 \\\n  )\n\n# Get maximum and minimum TotalRevenue values (consistent scale)\nz_min_val = res_df['TotalRevenue'].min()\nz_max_val = res_df['TotalRevenue'].max()\n\n# Create frames (one per year, and aggregate)\nframes = []\n\nfor year in years:\n  df_year = res_df[res_df['Year'] == year]\n  frames.append(go.Frame(\n    name = str(year),\n    data = [go.Choropleth(\n      locations = df_year['iso_alpha'],\n      z = df_year['TotalRevenue'],\n      zmin = z_min_val,\n      zmax = z_max_val,\n      text = df_year['hover_text'],\n      hoverinfo = 'text',\n      # Color-blind friendly color scale (reversed: darker with higher revenues)\n      colorscale = 'Viridis_r',\n      # Give national boundaries a dark grey outline\n      marker = dict(line=dict(color='darkgrey', width=0.5))\n    )]\n  ))\n  \n# First frame (initial state)\ninit_df = res_df[res_df['Year'] == 'All']\n\n# Generate plotly Choropleth\nfig = go.Figure(\n  data=[go.Choropleth(\n        locations=init_df['iso_alpha'],\n        z=init_df['TotalRevenue'],\n        text=init_df['hover_text'],\n        hoverinfo='text',\n        # Color-blind friendly color scale (reversed: darker with higher revenues)\n        colorscale='Viridis_r',\n        zmin=z_min_val,\n        zmax=z_max_val,\n        # Give national boundaries a dark grey outline\n        marker=dict(line=dict(color='darkgrey', width=0.5)),\n        # Title Colorbar/Legend\n        colorbar=dict(title='Total Revenue (USD$)')\n    )],\n    frames=frames\n  )\n  \n# Format Layout with Animation Controls\nfig.update_layout(\n  title = dict(\n    text = \"Total Revenue by Country &lt;br&gt; 2009-01-01 to 2013-12-22\",\n    x = 0.5,\n    xanchor = 'center',\n    font = dict(size=18)\n  ),\n  margin=dict(t=80),\n  # Frame and view\n  geo = dict(\n    # Show countries and boundaries\n    showcountries = True,\n    # Give national boundaries a dark gray outline\n    countrycolor=\"darkgrey\",\n    # Add coast lines - ensure scountries that aren't in data are seen\n    showcoastlines = True,\n    coastlinecolor = \"gray\",\n    #  Ad a neat little frame around the world\n    showframe = True,\n    framecolor = \"black\",\n    # Use natural earth projection\n    projection_type = \"natural earth\"\n  ),\n  # Buttons/Menus\n  updatemenus = [dict(\n    ## Play/Pause\n        # First button active by default (yr == \"All\")\n        type = \"buttons\",\n        direction = \"left\",\n        x = 0,\n        y = 0,\n        showactive = False,\n        xanchor = \"left\",\n        yanchor = \"bottom\",\n        pad = dict(r = 10, t = 70),\n        buttons = [dict(\n          label = \"Play\",\n          method = \"animate\",\n          args = [None, {\n            \"frame\": {\"duration\": 1000, \"redraw\": True},\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}\n          }]\n        ), dict(\n          label = \"Pause\",\n          method = \"animate\",\n          args=[[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}] \n          )] \n      )] +\n  ## Year Dropdown Menu\n    [dict(\n      type=\"dropdown\",\n      x = 0.1,\n      y = 1.15,\n      xanchor=\"left\",\n      yanchor=\"top\",\n      showactive=True,\n      buttons=[dict(\n        label=str(year),\n        method=\"animate\",\n        args=[\n            [str(year)],\n            {\"mode\": \"immediate\",\n             \"frame\": {\"duration\": 0, \"redraw\": True},\n             \"transition\": {\"duration\": 0}}\n        ]\n    ) for year in years]\n  )],\n  sliders = [dict(\n      active = 0,\n      # Positioning of slider menu\n      x = 0.1,\n      y = -0.2,\n      len = 0.8,\n      xanchor = \"left\",\n      yanchor = \"bottom\",\n      pad = dict(t= 30, b=10),\n      currentvalue = dict(\n        visible = True,\n        prefix = \"Year: \",\n        xanchor = \"right\",\n        font = dict(size=14, color = \"#666\")\n      ),\n    steps = [dict(\n      method = 'animate',\n      args =[[str(year)], {\n        \"mode\": \"immediate\",\n        \"frame\": {\"duration\": 1000, \"redraw\": True},\n        \"transition\": {\"duration\": 300}\n        }],\n        label = str(year) \n      ) for year in years]\n    )] \n  );\n\n# Display interactive plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe customer base is consistently geographically uneven.\n\nRevenue trends remain flat overall.\nNorth America and Brazil consistently drive the highest revenues, indicating strong market presence and sustained demand.\nIndia and parts of Central Europe maintain low but steady revenue, suggesting a stable (though modest) consumer base that may respond well to targeted growth strategies.\nAustralia showed signs of growth until 2013, after which revenue dropped to zero — this may reflect market exit, operational changes, or demand saturation.\nOther parts of South America and Europe show sporadic revenue, possibly tied to one-time purchases or minimal customer engagement.\n\nOpportunities:\n\nThere is untapped potential in underrepresented regions. If market demand can be properly assessed and activated — through localized marketing, partnerships, or product adaptation — these regions could represent growth markets.\nSouth America and Europe may benefit from customer acquisition and retention strategies.\nAustralia may be a strong case-study: understanding why Australia stopped producing revenue may identify key vulnerabilities that might translate to other markets."
  }
]