[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome! Here you’ll find a small collection of personal projects that reflect some of my skills, interests, and growth. Feel free to explore — use the filters to browse by category and dive into the areas that interest you the most.\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)\n\n\n\nSQL\n\nR\n\nPython\n\nDashboard\n\nData Analysis\n\nBusiness Intelligence\n\nExploratory Analysis\n\nRetail Analytics\n\nChinook\n\nShort\n\nDuckDB\n\n\n\nThis project explores the Chinook dataset — a mock digital music store — to uncover key business insights around revenue, customers, and performance. It combines SQL analysis with dashboard development to present findings visually.\n\n\n\n\n\nJul 2, 2025\n\n\nMorrigan M.\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m mainly excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I was a critical care nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner.\n\n\n\n\nMPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology\n\n\n\n\n\nGraduate Research Assistant, February 2023 - Present\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\nCritical Care Travel Nurse, March 2021 - December 2022\nTravel Nurse Across America (USA)\n\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\nRegistered Nurse (Residency), March 2020 - March 2021\nRochester General Hospital (Rochester, NY, USA)\nPublic Safety Communications Specialist, February 2018 - February 2020\nCity of Plano (Plano, TX, USA)"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m mainly excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I was a critical care nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Morri",
    "section": "",
    "text": "MPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Morri",
    "section": "",
    "text": "Graduate Research Assistant, February 2023 - Present\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\nCritical Care Travel Nurse, March 2021 - December 2022\nTravel Nurse Across America (USA)\n\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\nRegistered Nurse (Residency), March 2020 - March 2021\nRochester General Hospital (Rochester, NY, USA)\nPublic Safety Communications Specialist, February 2018 - February 2020\nCity of Plano (Plano, TX, USA)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m especially excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I spent over a decade as a nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner. (Also: the dog’s name is Lucy. She does not code.)\n\n\n\n\nMPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology\n\n\n\n\n\n\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\n\n\n\nTravel Nurse Across America, USA\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\n\n\n\nRochester General Hospital, Rochester, NY, USA\n\n\n\nCity of Plano, Plano, TX, USA"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m especially excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I spent over a decade as a nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner. (Also: the dog’s name is Lucy. She does not code.)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Morri",
    "section": "",
    "text": "MPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About Morri",
    "section": "",
    "text": "UTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\n\n\n\nTravel Nurse Across America, USA\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\n\n\n\nRochester General Hospital, Rochester, NY, USA\n\n\n\nCity of Plano, Plano, TX, USA"
  },
  {
    "objectID": "projects/2025_bi_chinook.html",
    "href": "projects/2025_bi_chinook.html",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "Goal: Build a BI dashboard for a fictional digital media business.\nFocus: Revenue insights, customer analysis, business performance.\nWhy it matters: Business stakeholders benefit from fast, accessible reporting on key performance indicators (KPIs) to support decision-making.\n\nThis project is intended for both technical and business audiences, with clear visual insights supported by SQL-based exploration.\nFor the best visualization experience, browser is recommended - optimization of these exploratory visualizations is limited.\n\n\nThis project was selected for its relevance to real-world business intelligence scenarios and its flexibility as a learning and demonstration tool. The Chinook dataset provides a realistic simulation of sales and customer data, enabling exploration of KPIs commonly used in industries such as retail, e-commerce, and SaaS.\nKey focus areas include:\n\nEnd-to-end analytical workflow from data exploration in DuckDB to dashboard construction—highlighting technical fluency and analytical structure.\nEmphasis on stakeholder communication showcasing the ability to translate complex queries into clear, business-oriented narratives.\nBroad applicability with insights around customer retention, regional performance, and product demand transferable across diverse verticals.\n\nBy combining strong SQL capabilities with clear business insight, this project demonstrates how structured data exploration can inform decisions and strategy at scale.\n\n\n\nThis project aims to provide insights on the following key business questions, with a time-based perspective embedded throughout each analysis to identify trends and changes over time:\n\nWhere is revenue coming from geographically?\nWhat genres or artists generate the most income?\nHow many customers are repeat buyers?\n\n\n\n\n\nSQL-based analysis of key business metrics with visualizations in both R and Python.\nDashboards built in R (Shiny) and Python (Dash) [TO DO].\n[Considering a tableau version of the dashboard; TO DO]\n\n\n\n\nThe following tools were used to merge, analyze, and present the data:\n\n\n\nTool\nPurpose\n\n\n\n\nSQL\nData transformation and KPIs\n\n\nDuckDB\nLightweight, embedded relational database\n\n\nR + Shiny\nDashboard development (R-based)\n\n\nPython + Dash\nDashboard development (Python-based)\n\n\n\nThis project demonstrates dashboard development in both R (Shiny) and Python (Dash), highlighting flexibility across technical ecosystems.\n[NOTE TO SELF - DELETE ONCE REVISED: This is a work in progress. I plan to make a dashboard with R and one with Python, just to show I can do both. I will update this to reflect the final project once it’s done.]\n\nR PackagesPython Packages\n\n\n\n\nShow Code\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(countrycode)\nlibrary(plotly)\nlibrary(ggplot2)\n\n\n\n\n\n\nShow Code\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport pycountry\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.graph_objs import Figure\nfrom plotly.colors import sample_colorscale"
  },
  {
    "objectID": "projects/2025_bi_chinook.html#why-this-project",
    "href": "projects/2025_bi_chinook.html#why-this-project",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "This project was selected for its relevance to real-world business intelligence scenarios and its flexibility as a learning and demonstration tool. The Chinook dataset provides a realistic simulation of sales and customer data, enabling exploration of KPIs commonly used in industries such as retail, e-commerce, and SaaS.\nKey focus areas include:\n\nEnd-to-end analytical workflow from data exploration in DuckDB to dashboard construction—highlighting technical fluency and analytical structure.\nEmphasis on stakeholder communication showcasing the ability to translate complex queries into clear, business-oriented narratives.\nBroad applicability with insights around customer retention, regional performance, and product demand transferable across diverse verticals.\n\nBy combining strong SQL capabilities with clear business insight, this project demonstrates how structured data exploration can inform decisions and strategy at scale."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#business-questions",
    "href": "projects/2025_bi_chinook.html#business-questions",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "This project aims to provide insights on the following key business questions, with a time-based perspective embedded throughout each analysis to identify trends and changes over time:\n\nWhere is revenue coming from geographically?\nWhat genres or artists generate the most income?\nHow many customers are repeat buyers?"
  },
  {
    "objectID": "projects/2025_bi_chinook.html#deliverables",
    "href": "projects/2025_bi_chinook.html#deliverables",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "SQL-based analysis of key business metrics with visualizations in both R and Python.\nDashboards built in R (Shiny) and Python (Dash) [TO DO].\n[Considering a tableau version of the dashboard; TO DO]"
  },
  {
    "objectID": "projects/2025_bi_chinook.html#tech-stack",
    "href": "projects/2025_bi_chinook.html#tech-stack",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "The following tools were used to merge, analyze, and present the data:\n\n\n\nTool\nPurpose\n\n\n\n\nSQL\nData transformation and KPIs\n\n\nDuckDB\nLightweight, embedded relational database\n\n\nR + Shiny\nDashboard development (R-based)\n\n\nPython + Dash\nDashboard development (Python-based)\n\n\n\nThis project demonstrates dashboard development in both R (Shiny) and Python (Dash), highlighting flexibility across technical ecosystems.\n[NOTE TO SELF - DELETE ONCE REVISED: This is a work in progress. I plan to make a dashboard with R and one with Python, just to show I can do both. I will update this to reflect the final project once it’s done.]\n\nR PackagesPython Packages\n\n\n\n\nShow Code\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(countrycode)\nlibrary(plotly)\nlibrary(ggplot2)\n\n\n\n\n\n\nShow Code\nimport duckdb\nimport numpy as np\nimport pandas as pd\nimport pycountry\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.graph_objs import Figure\nfrom plotly.colors import sample_colorscale"
  },
  {
    "objectID": "projects/2025_bi_chinook.html#connection-validation",
    "href": "projects/2025_bi_chinook.html#connection-validation",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Connection and Validation",
    "text": "Connection and Validation\nA connection was made to the DuckDB database file.\n\nRPython\n\n\n\n\nShow Code\ncon_chinook &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"../data/chinook.duckdb\",\n  read_only = TRUE\n  )\n\n\n\n\n\n\nShow Code\ncon_chinook = duckdb.connect(\"../data/chinook.duckdb\", read_only = True)\n\n\n\n\n\nInitial exploration confirmed the data’s structure and date range matched documentation.\nA query of date values in the InvoiceDate table confirmed that the data contained records with a date range from 2009-01-01 to 2013-12-22.\n\n\n\nShow Code\n\nSQL\n\n-- Get date range of Invoices\nSELECT \n  MIN(i.InvoiceDate) as MinDate, \n  MAX(i.InvoiceDate) as MaxDate\nFROM Invoice i;\n\n\n\n\n1 records\n\n\nMinDate\nMaxDate\n\n\n\n\n2009-01-01\n2013-12-22\n\n\n\n\n\nAs expected, InvoiceLine and Track had the highest number of unique records, reflecting their one-to-many relationships with Invoice and Album, respectively. Metadata tables such as Genre and MediaType had fewer unique values.\n\n\n\nShow Code\n\nSQL\n\n-- Get Number of Unique Key Values in Each Table\nSELECT \n  'Employees' AS TableName, \n  COUNT(DISTINCT EmployeeId) AS UniqueKeys \nFROM Employee\nUNION ALL\nSELECT \n  'Customers' AS TableName, \n  COUNT(DISTINCT Customerid) AS UniqueKeys \nFROM Customer\nUNION ALL\nSELECT \n  'Invoices' AS TableName, \n  COUNT(DISTINCT InvoiceId) AS UniqueKeys \nFROM Invoice\nUNION ALL\nSELECT \n  'Invoice Lines' AS TableName, \n  COUNT(DISTINCT InvoiceLineId) AS UniqueKeys \nFROM InvoiceLine\nUNION ALL\nSELECT \n  'Tracks' AS TableName, \n  COUNT(DISTINCT TrackId) AS UniqueKeys \nFROM Track\nUNION ALL\nSELECT \n  'Artists' AS TableName, \n  COUNT(DISTINCT ArtistId) AS UniqueKeys \nFROM Artist\nUNION ALL\nSELECT \n  'Albums' AS TableName, \n  COUNT(DISTINCT AlbumId) AS UniqueKeys \nFROM Album\nUNION ALL\nSELECT \n  'Genres' AS TableName, \n  COUNT(DISTINCT GenreId) AS UniqueKeys \nFROM Genre\nUNION ALL\nSELECT \n  'Media Types' AS TableName,\n  COUNT(DISTINCT MediaTypeId) AS UniqueKeys \nFROM MediaType\nUNION ALL\nSELECT \n  'Playlists' AS TableName, \n  COUNT(DISTINCT PlaylistId) AS UniqueKeys \nFROM Playlist\nORDER BY UniqueKeys DESC;\n\n\n\n\nDisplaying records 1 - 10\n\n\nTableName\nUniqueKeys\n\n\n\n\nTracks\n3503\n\n\nInvoice Lines\n2240\n\n\nInvoices\n412\n\n\nAlbums\n347\n\n\nArtists\n275\n\n\nCustomers\n59\n\n\nGenres\n25\n\n\nPlaylists\n18\n\n\nEmployees\n8\n\n\nMedia Types\n5\n\n\n\n\n\nNext Step: Key performance indicators (KPIs) will be extracted with SQL and visualized with R and Python to answer the business questions outlined previously, beginning with a geographic revenue analysis."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#q1-vis",
    "href": "projects/2025_bi_chinook.html#q1-vis",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Exploratory Visualizations",
    "text": "Exploratory Visualizations\nIn preparation for exploratory visualization generation, the data was retrieved using SQL queries and prepared in both R and Python.\n\nRPython\n\n\n\n\nShow Code\n# SQL Queries\n## Yearly Breakdown\nres_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n    -- Get Country and Year for grouping\n    i.BillingCountry as Country, \n    YEAR(i.InvoiceDate) as Year,\n    -- Calculate Total Revenue\n    SUM(i.Total) AS TotalRevenue,\n    -- Calculate % of Total/Global Revenue\n    ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n    -- Get Number of Customers\n    COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n    -- Calculate Revenue per Customer\n    ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\nFROM Customer c\nJOIN Invoice i on c.CustomerId == i.CustomerId\nGROUP BY i.BillingCountry, Year\n-- Sort Revenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n## Total (all years)\nres_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n    -- Get Country for grouping\n    i.BillingCountry as Country,\n    -- Set 'Year' to 'All' for grouping\n    'All' AS Year,\n    -- Calculate Total Revenue\n    SUM(i.Total) AS TotalRevenue,\n    -- Calculate % of Total/Global Revenue\n    ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n    -- Get Number of Customers\n    COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n    -- Calculate Revenue per Customer\n    ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\nFROM Customer c\nJOIN Invoice i on c.CustomerId == i.CustomerId\nGROUP BY i.BillingCountry,\n-- Sort Revenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n# Combine data frames in R\nres_df &lt;- dplyr::bind_rows(\n  res_agg_df,\n  res_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  ) |&gt;\n  dplyr::mutate(\n    ## Add ISO Country Codes\n    iso_alpha = countrycode::countrycode(\n      Country, \n      origin = 'country.name', \n      destination = 'iso3c'\n      ),\n    ## Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $TotalRevenue.##\")\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(TotalRevenue, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get vector of unique years (layers/traces) - order with \"All\" first.\nyears &lt;- c(\"All\", sort(unique(res_yearly_df$Year)))\n\n\n\n\n\n\nShow Code\n# SQL Queries\n## Yearly Breakdown\nres_yearly_df = con_chinook.execute(\n    \"\"\"SELECT \n      -- Get Country and Year for grouping\n      i.BillingCountry as Country, \n      YEAR(i.InvoiceDate) as Year,\n      -- Calculate Total Revenue\n      SUM(i.Total) AS TotalRevenue,\n      -- Calculate % of Total/Global Revenue\n      ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n      -- Get Number of Customers\n      COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n      -- Calculate Revenue per Customer\n      ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\n  FROM Customer c\n  JOIN Invoice i on c.CustomerId == i.CustomerId\n  GROUP BY i.BillingCountry, Year\n  -- Sort Revenue (Highest to Lowest)\n  ORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n\n## Total (all years)\nres_agg_df = con_chinook.execute(\n    \"\"\"SELECT \n      -- Get Country for grouping\n      i.BillingCountry as Country,\n      -- Set 'Year' to 'All' for grouping\n      'All' AS Year,\n      -- Calculate Total Revenue\n      SUM(i.Total) AS TotalRevenue,\n      -- Calculate % of Total/Global Revenue\n      ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n      -- Get Number of Customers\n      COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n      -- Calculate Revenue per Customer\n      ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\n  FROM Customer c\n  JOIN Invoice i on c.CustomerId == i.CustomerId\n  GROUP BY i.BillingCountry,\n  -- Sort Revenue (Highest to Lowest)\n  ORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n\n# Combine data frames and ensure consistent types\nres_df = pd.concat([\n  res_agg_df,\n  res_yearly_df.assign(Year=res_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n\n# Add ISO Country Codes\ndef get_iso_alpha3(country_name):\n try:\n   match = pycountry.countries.search_fuzzy(country_name)\n   return match[0].alpha_3\n except LookupError:\n   return None\n \nres_df['iso_alpha'] = res_df['Country'].apply(get_iso_alpha3)\n\n# Get unique years (layers/traces) - order with \"All\" first.\nyears = [\"All\"] + sorted(res_df[res_df['Year'] != 'All']['Year'].unique().tolist())\n\n\n\n\n\n\nTotal Revenue: Choropleth by Country\nTotal revenue per country across the entire data set was visualized with a Choropeth plot.\n\nRPython\n\n\n\n\nShow Code\n# Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $TotalRevenue.##\")\nres_df &lt;- res_df |&gt; \n  dplyr::mutate(\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(TotalRevenue, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get minimum and maximum values for TotalRevenue (Colorbar consistency)\nz_min_val &lt;- min(res_df$TotalRevenue, na.rm = TRUE)\nz_max_val &lt;- max(res_df$TotalRevenue, na.rm = TRUE)\n\n# Generate plotly Choropleth\nfig &lt;- plotly::plot_ly(\n  data = res_df,\n  type = 'choropleth',\n  locations = ~iso_alpha,\n  z = ~TotalRevenue,\n  # Set hover text to only display our desired, formatted output\n  text = ~hover_text,\n  hoverinfo = \"text\",\n  frame = ~Year,\n  # Set minimum and maximum TotalRevenue values, for consistent scale\n  zmin = z_min_val,\n  zmax = z_max_val,\n  # Title Colorbar/Legend\n  colorbar = list(\n    title = \"Total Revenue (USD$)\"\n  ),\n  # Color-blind friendly color scale\n  colorscale = \"Viridis\",\n  reversescale = TRUE,\n  showscale = TRUE,\n  # Give national boundaries a dark gray outline\n  marker = list(line = list(color = \"darkgrey\", width = 0.5))\n)\n\n# Layout with animation controls\nfig &lt;- fig %&gt;%\n  plotly::layout(\n    title = list(\n      text = \"Total Revenue by Country &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n      x = 0.5,\n      xanchor = \"center\",\n      font = list(size = 18)\n    ),\n    geo = list(\n      # Add a neat little frame around the world\n      showframe = TRUE, \n      # Add coast lines - ensures countries that aren't in data are seen\n      showcoastlines = TRUE, \n      # Use natural earth projection\n      projection = list(type = 'natural earth')\n      ),\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        showactive = TRUE,\n        buttons = purrr::map(years, function(yr) {\n          list(\n            method = \"animate\",\n            args = list(list(yr), list(mode = \"immediate\", frame = list(duration = 0, redraw = TRUE))),\n            label = yr\n          )\n        }),\n        # Positioning of dropdown menu\n        x = 0.1,\n        y = 1.15,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    ),\n    margin = list(t = 80)\n  ) %&gt;%\n  plotly::animation_opts(frame = 1000, transition = 0, redraw = TRUE)\n\n# Display interactive plot\nfig\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Specify hover text\nres_df['hover_text'] = res_df.apply( \\\n  lambda row: f\"&lt;b&gt;{row['Country']}&lt;/b&gt;&lt;br&gt; ${row['TotalRevenue']:.2f}\", axis = 1 \\\n  )\n\n# Get maximum and minimum TotalRevenue values (consistent scale)\nz_min_val = res_df['TotalRevenue'].min()\nz_max_val = res_df['TotalRevenue'].max()\n\n# Create frames (one per year, and aggregate)\nframes = []\n\nfor year in years:\n  df_year = res_df[res_df['Year'] == year]\n  frames.append(go.Frame(\n    name = str(year),\n    data = [go.Choropleth(\n      locations = df_year['iso_alpha'],\n      z = df_year['TotalRevenue'],\n      zmin = z_min_val,\n      zmax = z_max_val,\n      text = df_year['hover_text'],\n      hoverinfo = 'text',\n      # Color-blind friendly color scale (reversed: darker with higher revenues)\n      colorscale = 'Viridis_r',\n      # Give national boundaries a dark grey outline\n      marker = dict(line=dict(color='darkgrey', width=0.5))\n    )]\n  ))\n  \n# First frame (initial state)\ninit_df = res_df[res_df['Year'] == 'All']\n\n# Generate plotly Choropleth\nfig = go.Figure(\n  data=[go.Choropleth(\n        locations=init_df['iso_alpha'],\n        z=init_df['TotalRevenue'],\n        text=init_df['hover_text'],\n        hoverinfo='text',\n        # Color-blind friendly color scale (reversed: darker with higher revenues)\n        colorscale='Viridis_r',\n        zmin=z_min_val,\n        zmax=z_max_val,\n        # Give national boundaries a dark grey outline\n        marker=dict(line=dict(color='darkgrey', width=0.5)),\n        # Title Colorbar/Legend\n        colorbar=dict(title='Total Revenue (USD$)')\n    )],\n    frames=frames\n  )\n  \n# Format Layout with Animation Controls\nfig.update_layout(\n  title = dict(\n    text = \"Total Revenue by Country &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n    x = 0.5,\n    xanchor = 'center',\n    font = dict(size=18)\n  ),\n  margin=dict(t=80),\n  # Frame and view\n  geo = dict(\n    # Show countries and boundaries\n    showcountries = True,\n    # Give national boundaries a dark gray outline\n    countrycolor=\"darkgrey\",\n    # Add coast lines - ensure countries that aren't in data are seen\n    showcoastlines = True,\n    coastlinecolor = \"gray\",\n    #  Ad a neat little frame around the world\n    showframe = True,\n    framecolor = \"black\",\n    # Use natural earth projection\n    projection_type = \"natural earth\"\n  ),\n  # Buttons/Menus\n  updatemenus = [dict(\n    ## Play/Pause\n        # First button active by default (yr == \"All\")\n        type = \"buttons\",\n        direction = \"left\",\n        x = 0,\n        y = 0,\n        showactive = False,\n        xanchor = \"left\",\n        yanchor = \"bottom\",\n        pad = dict(r = 10, t = 70),\n        buttons = [dict(\n          label = \"Play\",\n          method = \"animate\",\n          args = [None, {\n            \"frame\": {\"duration\": 1000, \"redraw\": True},\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}\n          }]\n        ), dict(\n          label = \"Pause\",\n          method = \"animate\",\n          args=[[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}] \n          )] \n      )] +\n  ## Year Dropdown Menu\n    [dict(\n      type=\"dropdown\",\n      x = 0.1,\n      y = 1.15,\n      xanchor=\"left\",\n      yanchor=\"top\",\n      showactive=True,\n      buttons=[dict(\n        label=str(year),\n        method=\"animate\",\n        args=[\n            [str(year)],\n            {\"mode\": \"immediate\",\n             \"frame\": {\"duration\": 0, \"redraw\": True},\n             \"transition\": {\"duration\": 0}}\n        ]\n    ) for year in years]\n  )],\n  sliders = [dict(\n      active = 0,\n      # Positioning of slider menu\n      x = 0.1,\n      y = -0.2,\n      len = 0.8,\n      xanchor = \"left\",\n      yanchor = \"bottom\",\n      pad = dict(t= 30, b=10),\n      currentvalue = dict(\n        visible = True,\n        prefix = \"Year: \",\n        xanchor = \"right\",\n        font = dict(size=14, color = \"#666\")\n      ),\n    steps = [dict(\n      method = 'animate',\n      args =[[str(year)], {\n        \"mode\": \"immediate\",\n        \"frame\": {\"duration\": 1000, \"redraw\": True},\n        \"transition\": {\"duration\": 300}\n        }],\n        label = str(year) \n      ) for year in years]\n    )] \n  );\n\n# Display interactive plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe customer base exhibits persistent geographic disparities in both scale and engagement.\n\nOverall revenue trends are relatively flat, with limited signs of organic growth.\nNorth America and Brazil consistently drive the highest revenues, indicating strong market presence and sustained demand.\nIndia and parts of Central Europe maintain modest but consistent revenue, suggesting a stable (though modest) consumer base that may respond well to targeted growth strategies.\nAustralia showed signs of growth until 2013, after which revenue dropped to zero — this may reflect market exit, operational changes, or demand saturation.\nOther parts of South America and Europe show sporadic revenue, possibly tied to one-time purchases or minimal customer engagement.\n\nOpportunities:\n\nThere is untapped potential in underrepresented regions. If market demand can be properly assessed and activated — through localized marketing, partnerships, or product adaptation — these regions could represent growth markets.\nSouth America and Europe may benefit from customer acquisition and retention strategies.\nAustralia’s sharp revenue drop in 2013 warrants deeper investigation — identifying root causes could help preempt similar risks in other markets.\n\n\n\n\n\nRevenue per Customer: Choropleth by Country\nInitial exploration revealed a significant mismatch between total revenue and revenue per customer. This was further explored by forming a similar Choropeth plot.\n\nRPython\n\n\n\n\nShow Code\n# Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $RevenuePerCustomer.##\")\nres_df &lt;- res_df |&gt; \n  dplyr::mutate(\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(RevenuePerCustomer, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get minimum and maximum values for RevenuePerCustomer (Colorbar consistency)\nz_min_val &lt;- min(res_df$RevenuePerCustomer, na.rm = TRUE)\nz_max_val &lt;- max(res_df$RevenuePerCustomer, na.rm = TRUE)\n\n# Generate plotly Choropleth\nfig &lt;- plotly::plot_ly(\n  data = res_df,\n  type = 'choropleth',\n  locations = ~iso_alpha,\n  z = ~RevenuePerCustomer,\n  # Set hover text to only display our desired, formatted output\n  text = ~hover_text,\n  hoverinfo = \"text\",\n  frame = ~Year,\n  # Set minimum and maximum RevenuePerCustomer values, for consistent scale\n  zmin = z_min_val,\n  zmax = z_max_val,\n  # Title Colorbar/Legend\n  colorbar = list(\n    title = \"Revenue per Customer (USD$)\"\n  ),\n  # Color-blind friendly color scale\n  colorscale = \"Viridis\",\n  reversescale = TRUE,\n  showscale = TRUE,\n  # Give national boundaries a dark gray outline\n  marker = list(line = list(color = \"darkgrey\", width = 0.5))\n)\n\n# Layout with animation controls\nfig &lt;- fig %&gt;%\n  plotly::layout(\n    title = list(\n      text = \"Revenue per Customer by Country &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n      x = 0.5,\n      xanchor = \"center\",\n      font = list(size = 18)\n    ),\n    geo = list(\n      # Add a neat little frame around the world\n      showframe = TRUE, \n      # Add coast lines - ensures countries that aren't in data are seen\n      showcoastlines = TRUE, \n      # Use natural earth projection\n      projection = list(type = 'natural earth')\n      ),\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        showactive = TRUE,\n        buttons = purrr::map(years, function(yr) {\n          list(\n            method = \"animate\",\n            args = list(list(yr), list(mode = \"immediate\", frame = list(duration = 0, redraw = TRUE))),\n            label = yr\n          )\n        }),\n        # Positioning of dropdown menu\n        x = 0.1,\n        y = 1.15,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    ),\n    margin = list(t = 80)\n  ) %&gt;%\n  plotly::animation_opts(frame = 1000, transition = 0, redraw = TRUE)\n\n# Display interactive plot\nfig\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Specify hover text\nres_df['hover_text'] = res_df.apply( \\\n  lambda row: f\"&lt;b&gt;{row['Country']}&lt;/b&gt;&lt;br&gt; ${row['RevenuePerCustomer']:.2f}\", axis = 1 \\\n  )\n\n# Get maximum and minimum RevenuePerCustomer values (consistent scale)\nz_min_val = res_df['RevenuePerCustomer'].min()\nz_max_val = res_df['RevenuePerCustomer'].max()\n\n# Create frames (one per year, and aggregate)\nframes = []\n\nfor year in years:\n  df_year = res_df[res_df['Year'] == year]\n  frames.append(go.Frame(\n    name = str(year),\n    data = [go.Choropleth(\n      locations = df_year['iso_alpha'],\n      z = df_year['RevenuePerCustomer'],\n      zmin = z_min_val,\n      zmax = z_max_val,\n      text = df_year['hover_text'],\n      hoverinfo = 'text',\n      # Color-blind friendly color scale (reversed: darker with higher revenues)\n      colorscale = 'Viridis_r',\n      # Give national boundaries a dark grey outline\n      marker = dict(line=dict(color='darkgrey', width=0.5))\n    )]\n  ))\n  \n# First frame (initial state)\ninit_df = res_df[res_df['Year'] == 'All']\n\n# Generate plotly Choropleth\nfig = go.Figure(\n  data=[go.Choropleth(\n        locations=init_df['iso_alpha'],\n        z=init_df['RevenuePerCustomer'],\n        text=init_df['hover_text'],\n        hoverinfo='text',\n        # Color-blind friendly color scale (reversed: darker with higher revenues)\n        colorscale='Viridis_r',\n        zmin=z_min_val,\n        zmax=z_max_val,\n        # Give national boundaries a dark grey outline\n        marker=dict(line=dict(color='darkgrey', width=0.5)),\n        # Title Colorbar/Legend\n        colorbar=dict(title='Revenue per Customer (USD$)')\n    )],\n    frames=frames\n  )\n  \n# Format Layout with Animation Controls\nfig.update_layout(\n  title = dict(\n    text = \"Revenue per Customer by Country &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n    x = 0.5,\n    xanchor = 'center',\n    font = dict(size=18)\n  ),\n  margin=dict(t=80),\n  # Frame and view\n  geo = dict(\n    # Show countries and boundaries\n    showcountries = True,\n    # Give national boundaries a dark gray outline\n    countrycolor=\"darkgrey\",\n    # Add coast lines - ensure countries that aren't in data are seen\n    showcoastlines = True,\n    coastlinecolor = \"gray\",\n    #  Ad a neat little frame around the world\n    showframe = True,\n    framecolor = \"black\",\n    # Use natural earth projection\n    projection_type = \"natural earth\"\n  ),\n  # Buttons/Menus\n  updatemenus = [dict(\n    ## Play/Pause\n        # First button active by default (yr == \"All\")\n        type = \"buttons\",\n        direction = \"left\",\n        x = 0,\n        y = 0,\n        showactive = False,\n        xanchor = \"left\",\n        yanchor = \"bottom\",\n        pad = dict(r = 10, t = 70),\n        buttons = [dict(\n          label = \"Play\",\n          method = \"animate\",\n          args = [None, {\n            \"frame\": {\"duration\": 1000, \"redraw\": True},\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}\n          }]\n        ), dict(\n          label = \"Pause\",\n          method = \"animate\",\n          args=[[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}] \n          )] \n      )] +\n  ## Year Dropdown Menu\n    [dict(\n      type=\"dropdown\",\n      x = 0.1,\n      y = 1.15,\n      xanchor=\"left\",\n      yanchor=\"top\",\n      showactive=True,\n      buttons=[dict(\n        label=str(year),\n        method=\"animate\",\n        args=[\n            [str(year)],\n            {\"mode\": \"immediate\",\n             \"frame\": {\"duration\": 0, \"redraw\": True},\n             \"transition\": {\"duration\": 0}}\n        ]\n    ) for year in years]\n  )],\n  sliders = [dict(\n      active = 0,\n      # Positioning of slider menu\n      x = 0.1,\n      y = -0.2,\n      len = 0.8,\n      xanchor = \"left\",\n      yanchor = \"bottom\",\n      pad = dict(t= 30, b=10),\n      currentvalue = dict(\n        visible = True,\n        prefix = \"Year: \",\n        xanchor = \"right\",\n        font = dict(size=14, color = \"#666\")\n      ),\n    steps = [dict(\n      method = 'animate',\n      args =[[str(year)], {\n        \"mode\": \"immediate\",\n        \"frame\": {\"duration\": 1000, \"redraw\": True},\n        \"transition\": {\"duration\": 300}\n        }],\n        label = str(year) \n      ) for year in years]\n    )] \n  );\n\n# Display interactive plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nYear-over-year shifts in revenue per customer suggest evolving engagement patterns across regions.\n\nHigh total-revenue countries (USA, Canada, Brazil) and India show flat revenue per customer trends, indicating stable but possibly casual engagement.\nSouth America saw increases in revenue per customer in both 2010 and 2013, suggesting periods of heightened individual customer value — possibly driven by regional promotions or market-specific trends.\nEurope experienced a spike in 2011, with multiple countries (e.g., Austria, Hungary, Ireland) showing high per-customer revenue. The cause is unclear but may reflect a surge in high-value purchases or regional campaigns\n\nOpportunities:\n\nHigh per-customer revenue regions (especially those with smaller total revenue footprints) may benefit from customer acquisition efforts, as existing users demonstrate strong engagement or purchasing behavior.\nFlat or declining per-customer revenue in large markets highlights a need for upselling, bundling, or personalized offers to increase customer lifetime value.\nInvestigating year-over-year anomalies (e.g., Australia’s 2012 spike, Europe in 2011) could uncover replicable growth levers or emerging market trends."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#q1-summary",
    "href": "projects/2025_bi_chinook.html#q1-summary",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Summary of Exploratory findings",
    "text": "Summary of Exploratory findings\nTotal revenue measures market size, while revenue per customer reflects intensity of engagement. Both are important for guiding different types of strategic decisions (e.g., acquisition vs. retention).\nGeographic revenue in the Chinook dataset is concentrated in a few strong markets, while many others remain underdeveloped. This disparity presents both risk and opportunity: efforts to deepen engagement in large casual markets (e.g., USA, Brazil) and expand in high-value but small markets (e.g., Austria, Chile) could lead to measurable revenue growth. Year-over-year changes — such as Australia’s exit and Europe’s 2011 spike — suggest that regional trends and operational shifts have meaningful financial impacts worth further investigation.\n\nTop 5 Countries by Total Revenue: USA, Canada, France, Brazil, Germany\nTop 5 Countries by Revenue per Customer: Chile, Hungary, Ireland, Czech Republic, Austria\n\n\n\n\n\n\n\nPossible Next Steps\n\n\n\nIf this were real-world data, the following actions could strengthen both analytical insight and strategic decision-making.\nDeepen the Analysis:\n\nInvestigate Australia’s 2012–13 drop-off.\nExamine customer churn, pricing changes, or external events that may explain the abrupt loss of revenue.\nAnalyze U.S. cohort churn to identify upsell opportunities.\nAs the largest revenue contributor with moderate revenue-per-customer, the U.S. market may contain untapped upsell or retention opportunities.\n\nStrategic Business Opportunities:\n\nLaunch targeted consumer acquisition campaigns in underpenetrated, high-potential markets.\nFocus on Chile, Hungary, the Czech Republic and Ireland — all show strong revenue-per-customer despite smaller customer bases.\nLocalize and invest in regional content for India and Asia.\nThese high-population markets show minimal engagement. Tailored content, pricing, or distribution could unlock substantial growth.\nRe-engage customers in stalled or declining regions.\nCountries like Brazil and Sweden once generated revenue but saw sustained drop-offs. A targeted win-back campaign could reclaim lapsed users."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#q2-vis",
    "href": "projects/2025_bi_chinook.html#q2-vis",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Exploratory Visualizations",
    "text": "Exploratory Visualizations\nIn preparation for exploratory visualization generation, the data was retrieved using SQL queries and prepared in both R and Python.\n\nRPython\n\n\n\n\nShow Code\n# SQL Queries\n## Genre\n### Yearly Breakdown\nres_genre_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\")\n\n## Aggregate\nres_genre_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\")\n\n## Artist\nres_artist_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\nres_artist_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n# Combine data frames in R\nres_genre_df &lt;- dplyr::bind_rows(\n  res_genre_agg_df,\n  res_genre_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  )\n\nres_artist_df &lt;- dplyr::bind_rows(\n  res_artist_agg_df,\n  res_artist_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  )\n\n# Get vector of unique years (layers/traces) - order with \"All\" first.\nyears &lt;- c(\"All\", sort(unique(res_genre_yearly_df$Year)))\n\n# Make \"Year\" an ordered factor (Plot generation ordering)\nres_genre_df &lt;- res_genre_df |&gt;\n  dplyr::mutate(Year = factor(Year, levels = years, ordered = TRUE))\n\nres_artist_df &lt;- res_artist_df |&gt;\n  dplyr::mutate(Year = factor(Year, levels = years, ordered = T))\n\n# Get vector of unique Genres - order by total revenue overall\ngenres &lt;- res_genre_agg_df |&gt;\n  dplyr::arrange(dplyr::desc(TotalRevenue)) |&gt;\n  dplyr::select(Genre) |&gt;\n  dplyr::distinct() |&gt; \n  dplyr::pull()\n\n# Make \"Genre\" an ordered factor (Plot generation ordering)\nres_genre_df &lt;- res_genre_df |&gt;\n  dplyr::mutate(\n    Genre = factor(Genre, levels = genres, ordered = T)\n  )\n\n# Get vector of unique Artists - order by total revenue overall\nartists &lt;- res_artist_agg_df |&gt;\n  dplyr::arrange(dplyr::desc(TotalRevenue)) |&gt;\n  dplyr::select(Artist) |&gt;\n  dplyr::distinct() |&gt; \n  dplyr::pull()\n\n# Make \"Genre\" an ordered factor (Plot generation ordering)\nres_artist_df &lt;- res_artist_df |&gt;\n  dplyr::mutate(\n    Genre = factor(Artist, levels = artists, ordered = T)\n  )\n\n\n\n\n\n\nShow Code\n# SQL Queries\n## Genre\n### Yearly Breakdown\nres_genre_yearly_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n### Aggregate\nres_genre_agg_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n## Arist\n### Yearly Breakdown\nres_artist_yearly_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n### Aggregate\nres_artist_agg_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n# Combine data frames and ensure consistent types\nres_genre_df = pd.concat([\n  res_genre_agg_df,\n  res_genre_yearly_df.assign(Year=res_genre_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n  \nres_artist_df = pd.concat([\n  res_artist_agg_df,\n  res_artist_yearly_df.assign(Year=res_artist_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n\n# Get unique years\nyears = [\"All\"] + sorted(res_genre_df[res_genre_df['Year'] != 'All']['Year'].unique().tolist())\n\n# Get unique genres, sorted by total revenue share\ngenres = (\n    res_genre_df[res_genre_df['Year'] == 'All']\n    .sort_values(by='TotalRevenue', ascending=False)['Genre']\n    .tolist()\n)\n\n## Make Genre an ordered categorical (for plots)\nres_genre_df['Genre'] = pd.Categorical(\n  res_genre_df['Genre'], \n  categories=genres, \n  ordered=True\n  )\n\nres_genre_df = res_genre_df.sort_values(['Genre', 'Year'])\n\n# Get unique artists, sorted by total revenue share\nartists = (\n    res_artist_df[res_artist_df['Year'] == 'All']\n    .sort_values(by='TotalRevenue', ascending=False)['Artist']\n    .tolist()\n)\n\n## Make Artist an ordered categorical (for plots)\nres_artist_df['Artist'] = pd.Categorical(\n  res_artist_df['Artist'], \n  categories=artists, \n  ordered=True\n  )\n  \nres_artist_df = res_artist_df.sort_values(['Artist', 'Year'])\n\n\n\n\n\n\nRevenue by Genre: Stacked Bar Chart\nTo better understand genre-level commercial performance, revenue was analyzed both in total and normalized by catalog size. This approach distinguished high-volume genres from those that generated more revenue per available track. The results were visualized with stacked plots to compare total earnings and catalog-adjusted efficiency over time.\n\nRPython\n\n\n\n\nShow Code\n# TO DO: Figure out how to get the x-axis values to show\n\n# Stacked Area Plot: Total Revenue by Genre\np1 &lt;- ggplot2::ggplot(\n  res_genre_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year =  forcats::fct_rev(Year)), \n  ggplot2::aes(x = Genre, y = TotalRevenue, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Total Revenue\",\n    x = \"Genre\",\n    y = \"Total Revenue (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Turn X-axis text horizontal (legibility)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5, hjust = 1)\n    )\n\n# Stacked Area Plot: Revenue per Track in Catalog by Genre\np2 &lt;- ggplot2::ggplot(\n  res_genre_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year = forcats::fct_rev(Year)), \n  ggplot2::aes(x = Genre, y = RevenuePerTotalTrack, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Revenue per Track in Catalog\",\n    x = \"Genre\",\n    y = \"Revenue per Track (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Remove X-axis label (large text & in prev plot)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x=element_blank()\n    )\n\n## Convert to interactive plotly plots\np1_int &lt;- plotly::ggplotly(p1)\np2_int &lt;- plotly::ggplotly(p2)\n\n# For p2, turn off legend for all traces (already in p1)\nfor (i in seq_along(p2_int$x$data)) {\n  p2_int$x$data[[i]]$showlegend &lt;- FALSE\n}\n\n# Combine plots and display\nplotly::subplot(\n  p1_int, \n  p2_int, \n  nrows = 2, \n  shareX = TRUE, \n  titleY = TRUE\n) %&gt;%\n  plotly::layout(\n    title = list(text = \"Revenue by Genre&lt;br&gt;&lt;sup&gt;2009-01-01 to 2013-12-22&lt;/sup&gt;\"),\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.1)\n  )\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Filter out \"All\" year\ndf = res_genre_df[res_genre_df[\"Year\"] != \"All\"]\n\n# Sort years in reverse\nyears_r = sorted(df[\"Year\"].unique(), reverse=True)\n\n# Use a function to suppress intermediate outputs from quarto rendering\ndef build_plot(df):\n\n  # Create subplot\n  fig = make_subplots(\n      rows=2, cols=1,\n      shared_xaxes=True,\n      vertical_spacing=0.08,\n      subplot_titles=(\"Total Revenue\", \"Revenue per Track in Catalog\")\n  )\n\n  # Colorblind-friendly color scheme\n  colors = sample_colorscale(\"Viridis\", [i/len(years_r) for i in range(len(years_r))])\n  \n  # Plot 1: Total Revenue\n  for i, year in enumerate(years_r):\n      subset = df[df[\"Year\"] == year]\n      fig.add_trace(\n          go.Bar(\n              x=subset[\"Genre\"],\n              y=subset[\"TotalRevenue\"],\n              name=str(year),\n              marker_color=colors[i],\n              showlegend=True if i == 0 else False  # only once for cleaner display\n          ),\n          row=1, col=1\n      )\n\n  # Plot 1: Revenue per Track in Catalog\n  for i, year in enumerate(years_r):\n      subset = df[df[\"Year\"] == year]\n      fig.add_trace(\n          go.Bar(\n              x=subset[\"Genre\"],\n              y=subset[\"RevenuePerTotalTrack\"],\n              name=str(year),\n              marker_color=colors[i],\n              showlegend=True  # show for all so full legend appears\n          ),\n          row=2, col=1\n      )\n\n  # Format Layout\n  fig.update_layout(\n    ## Make stacked bar plot\n      barmode='stack',\n      title=dict(\n          text='Revenue by Genre&lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;',\n          x=0.5,\n          xanchor='center',\n          font=dict(size=20)\n      ),\n      height=700,\n      margin=dict(t=120),\n      ## Legend on bottom, horizontal\n      legend=dict(\n          orientation='h',\n          yanchor='bottom',\n          y=-0.25,\n          xanchor='center',\n          x=0.5,\n          title='Year'\n      )\n  )\n\n  # Format axes\n  ## Y-axis as dollars\n  fig.update_yaxes(title_text=\"Total Revenue (USD)\", row=1, col=1, tickformat=\"$,.2f\")\n  fig.update_yaxes(title_text=\"Revenue per Track (USD)\", row=2, col=1, tickformat=\"$,.2f\")\n  ## Ensure only one x-axis shows, rotated (for legibility)\n  fig.update_xaxes(title_text=None, row=1, col=1)\n  fig.update_xaxes(title_text=\"Genre\", tickangle=45, row=2, col=1)\n\n  # return figure\n  return fig\n\n# Display plot\nfig = build_plot(df)\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nGenre-level revenue remains stable over time, with no significant year-over-year shifts. This aligns with earlier findings: a small number of genres — primarily Rock, along with Latin, Metal, Alternative & Punk, and TV Shows — consistently drive the majority of revenue.\nRevenue per track in catalog tells a different story. While Rock shows consistent but moderate returns (likely due to its large catalog volume), several niche genres outperform in efficiency:\n\nSci Fi & Fantasy shows disproportionately high revenue per track, largely driven by premium TV content such as Battlestar Galactica. This genre saw a strong spike in 2011–2012 before tapering off in 2013 — suggesting a temporary surge in popularity.\nComedy experienced an anomalous peak in 2012, attributable to sales of The Office episodes.\nBossa Nova displayed a notable jump in 2012 revenue per catalog track, indicating strong demand relative to catalog size. Though less pronounced, this trend continued into 2013, hinting at sustained niche appeal.\nTV Show performance is likely undervalued in the current database structure, as Battlestar Galactica and The Office (TV Shows classified as Sci Fi & Frantasy and Comedy, respectively) are drivers of high per-track revenue in genres that are otherwise unrepresented in total revenue.\n\n\nOpportunities:\n\nInvest in high-margin, premium content.\nTV-based genres such as Sci Fi & Fantasy and Comedy show strong per-track performance despite limited catalog sizes. Expanding these offerings — particularly episodic or narrative-driven media — could unlock outsized returns relative to content volume.\nCapitalize on short-term demand spikes.\nGenres like Bossa Nova and Comedy demonstrated temporal revenue surges. Identifying and acting on these trends through curated promotions, seasonal playlists, or limited-time bundles may help convert interest into sustained revenue.\nReevaluate catalog strategies for large-volume genres.\nWhile Rock contributes the most revenue overall, its performance per track is relatively modest. Optimizing underperforming catalog segments — through targeted marketing or re-packaging — may improve monetization efficiency without requiring new content.\nMonitor genre saturation and track penetrance.\nGenres with high sales penetration (e.g., Sci Fi & Fantasy) may signal either strong consumer interest or an exhausted catalog. Ensuring a consistent stream of new content in high-penetration genres can help maintain engagement and avoid plateaus.\n\n\n\n\n\nArtist Revenue\n\n\nRevenue by Artist: Stacked Bar Chart\nTo better understand artist-level commercial performance, revenue was analyzed both in total and normalized by catalog size. This approach distinguished high-volume artists from those that generated more revenue per available track. The results were visualized with stacked plots to compare total earnings and catalog-adjusted efficiency over time.\n\nRPython\n\n\n\n\nShow Code\n# TO DO: Figure out how to make the x-axis values show\n\n# Stacked Area Plot: Total Revenue by Artist\np1 &lt;- ggplot2::ggplot(\n  res_artist_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year =  forcats::fct_rev(Year)) |&gt;\n   ## Restrict to top 20 artists for legibility\n    dplyr::filter(Artist %in% artists[1:20]) |&gt;\n    dplyr::mutate(Artist = factor(Artist, levels = artists[1:20], ordered = T)), \n  ggplot2::aes(x = Artist, y = TotalRevenue, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Total Revenue\",\n    x = \"Artist\",\n    y = \"Total Revenue (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Turn X-axis text horizontal (legibility)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5, hjust = 1)\n    )\n\n# Stacked Area Plot: Revenue per Track in Catalog by Artist\np2 &lt;- ggplot2::ggplot(\n  res_artist_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year = forcats::fct_rev(Year)) |&gt;\n   ## Restrict to top 20 artists for legibility\n    dplyr::filter(Artist %in% artists[1:20]) |&gt;\n    dplyr::mutate(Artist = factor(Artist, levels = artists[1:20], ordered = T)), \n  ggplot2::aes(x = Artist, y = RevenuePerTotalTrack, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Revenue per Track in Catalog\",\n    x = \"Artist\",\n    y = \"Revenue per Track (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Remove X-axis label (large text & in prev plot)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x=element_blank()\n    )\n\n## Convert to interactive plotly plots\np1_int &lt;- plotly::ggplotly(p1)\np2_int &lt;- plotly::ggplotly(p2)\n\n# For p2, turn off legend for all traces (already in p1)\nfor (i in seq_along(p2_int$x$data)) {\n  p2_int$x$data[[i]]$showlegend &lt;- FALSE\n}\n\n# Combine plots and display\nplotly::subplot(\n  p1_int, \n  p2_int, \n  nrows = 2, \n  shareX = TRUE, \n  titleY = TRUE\n) %&gt;%\n  plotly::layout(\n    title = list(text = \"Revenue by Artist&lt;br&gt;&lt;sup&gt;2009-01-01 to 2013-12-22&lt;/sup&gt;\"),\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.1)\n  )\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Filter out \"All\" year\ndf = res_artist_df[res_artist_df[\"Year\"] != \"All\"]\n\n# Restrict to top 20 artists (for legibility)\ndf = df[df[\"Artist\"].isin(artists[0:20])]\n\n# Use a function to suppress intermediate outputs from quarto rendering\ndef build_plot(df):\n\n  # Sort years in reverse\n  years_r = sorted(df[\"Year\"].unique(), reverse=True)\n  \n  # Create subplot\n  fig = make_subplots(\n      rows=2, cols=1,\n      shared_xaxes=True,\n      vertical_spacing=0.08,\n      subplot_titles=(\"Total Revenue\", \"Revenue per Track in Catalog\")\n  )\n\n  # Colorblind-friendly color scheme\n  colors = sample_colorscale(\"Viridis\", [i/len(years_r) for i in range(len(years_r))])\n\n  # Plot 1: Total Revenue\n  for i, year in enumerate(years_r):\n      subset = df[df[\"Year\"] == year]\n      fig.add_trace(\n          go.Bar(\n              x=subset[\"Artist\"],\n              y=subset[\"TotalRevenue\"],\n              name=str(year),\n              marker_color=colors[i],\n              showlegend=True if i == 0 else False  # only once for cleaner display\n          ),\n          row=1, col=1\n      )\n\n  # Plot 1: Revenue per Track in Catalog\n  for i, year in enumerate(years_r):\n      subset = df[df[\"Year\"] == year]\n      fig.add_trace(\n          go.Bar(\n              x=subset[\"Artist\"],\n              y=subset[\"RevenuePerTotalTrack\"],\n              name=str(year),\n              marker_color=colors[i],\n              showlegend=True  # show for all so full legend appears\n          ),\n          row=2, col=1\n      )\n\n  # Format Layout\n  fig.update_layout(\n    ## Make stacked bar plot\n      barmode='stack',\n      title=dict(\n          text='Revenue by Artist&lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;',\n          x=0.5,\n          xanchor='center',\n          font=dict(size=20)\n      ),\n      height=700,\n      margin=dict(t=120),\n      ## Legend on bottom, horizontal\n      legend=dict(\n          orientation='h',\n          yanchor='bottom',\n          y=-0.25,\n          xanchor='center',\n          x=0.5,\n          title='Year'\n      )\n  )\n\n  # Format axes\n  ## Y-axis as dollars\n  fig.update_yaxes(title_text=\"Total Revenue (USD)\", row=1, col=1, tickformat=\"$,.2f\")\n  fig.update_yaxes(title_text=\"Revenue per Track (USD)\", row=2, col=1, tickformat=\"$,.2f\")\n  ## Ensure only one x-axis shows, rotated (for legibility)\n  fig.update_xaxes(title_text=None, row=1, col=1)\n  fig.update_xaxes(title_text=\"Artist\", tickangle=45, row=2, col=1)\n  \n  # return figure\n  return fig\n\n# Display plot\nfig = build_plot(df)\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nArtist revenue is temporally consistent.\nMost artists show stable annual revenue across the 2009–2013 period. While individual years may fluctuate (e.g., dips for Metallica and Led Zeppelin in 2011), these appear to be temporary rather than indicative of long-term trends.\nAverage revenue per track sold is remarkably stable (~$0.75).\nA small number of high-volume artists (e.g., Deep Purple, Pearl Jam, Van Halen) show lower per-track revenue (~$0.50), suggesting that large catalogs may dilute revenue unless paired with sustained demand. This could indicate catalog saturation or uneven consumer engagement across their offerings.\nRevenue per track in catalog highlights structural differences.\nThis metric remains consistent for most music artists, but TV show content (e.g., Lost, The Office) shows a different pattern: exceptionally high revenue per catalog track but concentrated in specific years. For instance, Lost peaked in 2012 before falling to zero in 2013, and The Office spiked from 2010–2012 with no revenue in adjacent years.\nTV Shows demonstrate high efficiency per asset.\nDespite small catalogs, TV content generates significantly higher revenue per track than traditional music. This suggests strong demand relative to supply — possibly pointing to unmet or episodic consumption patterns.\n\nOpportunities:\n\nExpand premium TV show content.\nThe high return per catalog item in TV genres suggests an underexploited category. Adding similar content (episodic, narrative-driven media) could yield high revenue efficiency without requiring large-scale production.\nReinvigorate large but underperforming catalogs.\nArtists with expansive libraries but low per-track revenue (e.g., Deep Purple, Pearl Jam) may benefit from targeted marketing efforts such as curated collections, remastered releases, or highlight reels that surface their most engaging content.\nUse consumption patterns to guide licensing and promotions.\nThe temporal spikes in shows like Lost and The Office suggest windows of elevated demand. Monitor and act on these seasonal or cultural surges to time promotions, exclusive bundles, or re-releases effectively.\nMonitor catalog saturation signals.\nWhen large catalogs yield diminishing per-track returns, it may be time to reassess content strategy. This includes retiring underperforming assets or introducing new content types that align better with current listener interests."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#q2-summary",
    "href": "projects/2025_bi_chinook.html#q2-summary",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Summary of Exploratory Findings",
    "text": "Summary of Exploratory Findings\nWhile overall genre revenue remains stable across years, genre-specific performance varies widely depending on how it’s measured:\n\nTotal Revenue reflects market size. Rock dominates here, alongside Latin, Metal, and Alternative.\nRevenue per Track in Catalog reveals efficiency. Sci Fi & Fantasy, Comedy, and Bossa Nova perform disproportionately well relative to their catalog sizes — often driven by episodic TV content.\n\nThese patterns suggest that catalog volume and monetization efficiency are often misaligned. Notably, the top-performing genres by revenue-per-track tend to be narrow in scope but high in engagement, hinting at an opportunity to rebalance the content portfolio.\nArtist level trends are similar, suggesting artists with expansive libraries but low per-track revenue may benefit from targeted marketing efforts such as curated collections, remastered releases, or highlight reels that surface their most engaging content.\n\nMost Efficient Genres by Revenue per Track (2012 spike): Sci Fi & Fantasy (Battlestar Galactica), Comedy (The Office), Bossa Nova\nTop Genres by Total Revenue: Rock, Latin, Metal, Alternative & Punk, TV Shows\n\n\n\n\n\n\n\nPossible Next Steps\n\n\n\nIf this were real-world data, the following actions could strengthen both analytical insight and strategic decision-making.\nDeepen the Analysis:\n\nAttribute-level analysis for top-performing genres.\nDisaggregate genre revenue by artist, album, or track to pinpoint the specific drivers of efficiency within high-performing niches.\nExplore customer-level purchase behavior.\nAre niche genre buyers also more valuable customers overall? Understanding cross-genre engagement can inform bundling or recommendation strategies.\n\nStrategic Business Opportunities:\n\nInvest in premium, episodic content.\nSci Fi & Fantasy and Comedy genres — driven by TV content — deliver strong returns per track. Expanding this segment could yield high-margin growth.\nReact quickly to demand spikes.\nTemporal surges in Bossa Nova and Comedy suggest that curated playlists or limited-time offers can help capitalize on emerging trends.\nReoptimize large, underperforming genres.\nRock has broad catalog coverage but moderate per-track performance. Marketing, repackaging, or removal of underperformers could improve efficiency.\nPrioritize catalog expansion where penetration is high.\nFor genres with high track sales penetration (e.g., Sci Fi), maintaining momentum may require fresh content to meet continuing demand."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#q3-vis",
    "href": "projects/2025_bi_chinook.html#q3-vis",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Exploratory Visualizations",
    "text": "Exploratory Visualizations\nIn preparation for exploratory visualization generation, the data was retrieved using SQL queries and prepared in both R and Python.\n\nRPython\n\n\n\n\nShow Code\n# SQL Queries\n## Retention Decay\nres_repeat_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"-- Customer repeat rate (monthly return behavior)\n  -- Obtain First Purchase Date (DuckDB required separate)\n  WITH FirstPurchase AS (\n      SELECT\n          CustomerId,\n          MIN(DATE(InvoiceDate)) AS FirstPurchaseDate\n      FROM Invoice\n      GROUP BY CustomerId\n  ),\n  \n  -- Get subsequent purchase date info from invoice\n  CustomerInvoices AS (\n      SELECT\n          i.CustomerId,\n          DATE(i.InvoiceDate) AS PurchaseDate,\n          fp.FirstPurchaseDate\n      FROM Invoice i\n      JOIN FirstPurchase fp ON i.CustomerId = fp.CustomerId\n  ),\n  \n  -- Calculate months since first purchase\n  RepeatInfo AS (\n      SELECT\n          CustomerId,\n          DATE_DIFF('month', FirstPurchaseDate, PurchaseDate) AS MonthsSinceFirst\n      FROM CustomerInvoices\n  )\n  \n  -- Count distinct returning customers per month offset\n  SELECT\n      MonthsSinceFirst,\n      COUNT(DISTINCT CustomerId) AS ReturningCustomers,\n      (SELECT COUNT(*) FROM FirstPurchase) AS CohortSize,\n      ROUND(100.0 * COUNT(DISTINCT CustomerId) * 1.0 / (SELECT COUNT(*) FROM FirstPurchase), 1) AS RetentionPct\n  FROM RepeatInfo\n  WHERE MonthsSinceFirst &gt; 0\n  GROUP BY MonthsSinceFirst\n  -- Arrange by 'per month offset' (Low to High)\n  ORDER BY MonthsSinceFirst;\"\n  )\n\n## Retention Across Cohorts\nres_cohort_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"-- Retention across cohorts\n  -- First purchase per customer and cohort assignment\n  WITH FirstPurchases AS (\n      SELECT\n          CustomerId,\n          MIN(DATE(InvoiceDate)) AS FirstPurchaseDate,\n          DATE_TRUNC('month', MIN(InvoiceDate)) AS CohortMonth\n      FROM Invoice\n      GROUP BY CustomerId\n  ),\n  \n  -- All purchases labeled with cohort info\n  AllPurchases AS (\n      SELECT\n          i.CustomerId,\n          DATE(i.InvoiceDate) AS PurchaseDate,\n          DATE_TRUNC('month', i.InvoiceDate) AS PurchaseMonth,\n          fp.CohortMonth,\n          DATE_DIFF('month', fp.FirstPurchaseDate, i.InvoiceDate) AS MonthsSinceFirst\n      FROM Invoice i\n      JOIN FirstPurchases fp ON i.CustomerId = fp.CustomerId\n  ),\n  \n  -- Cohort sizes\n  CohortSizes AS (\n      SELECT\n          CohortMonth,\n          COUNT(DISTINCT CustomerId) AS CohortSize\n      FROM FirstPurchases\n      GROUP BY CohortMonth\n  ),\n  \n  -- Distinct customer activity per cohort/month\n  CustomerActivity AS (\n      SELECT DISTINCT\n          CustomerId,\n          CohortMonth,\n          MonthsSinceFirst\n      FROM AllPurchases\n  )\n  \n  -- Final aggregation with cohort size and retention %\n  SELECT\n      ca.CohortMonth,\n      ca.MonthsSinceFirst,\n      COUNT(DISTINCT ca.CustomerId) AS NumActiveCustomers,\n      cs.CohortSize,\n      ROUND(100.0 * COUNT(DISTINCT ca.CustomerId)::DOUBLE / cs.CohortSize, 1) AS RetentionPct\n  FROM CustomerActivity ca\n  JOIN CohortSizes cs ON ca.CohortMonth = cs.CohortMonth\n  GROUP BY ca.CohortMonth, ca.MonthsSinceFirst, cs.CohortSize\n  ORDER BY ca.CohortMonth, ca.MonthsSinceFirst;\"\n  )\n\n\n\n\n\n\nShow Code\n# SQL Queries\n## Retention Decay\nres_repeat_df = con_chinook.execute(\n  \"\"\"-- Customer repeat rate (monthly return behavior)\n  -- Obtain First Purchase Date (DuckDB required separate)\n  WITH FirstPurchase AS (\n      SELECT\n          CustomerId,\n          MIN(DATE(InvoiceDate)) AS FirstPurchaseDate\n      FROM Invoice\n      GROUP BY CustomerId\n  ),\n  \n  -- Get subsequent purchase date info from invoice\n  CustomerInvoices AS (\n      SELECT\n          i.CustomerId,\n          DATE(i.InvoiceDate) AS PurchaseDate,\n          fp.FirstPurchaseDate\n      FROM Invoice i\n      JOIN FirstPurchase fp ON i.CustomerId = fp.CustomerId\n  ),\n  \n  -- Calculate months since first purchase\n  RepeatInfo AS (\n      SELECT\n          CustomerId,\n          DATE_DIFF('month', FirstPurchaseDate, PurchaseDate) AS MonthsSinceFirst\n      FROM CustomerInvoices\n  )\n  \n  -- Count distinct returning customers per month offset\n  SELECT\n      MonthsSinceFirst,\n      COUNT(DISTINCT CustomerId) AS ReturningCustomers,\n      (SELECT COUNT(*) FROM FirstPurchase) AS CohortSize,\n      ROUND(100.0 * COUNT(DISTINCT CustomerId) * 1.0 / (SELECT COUNT(*) FROM FirstPurchase), 1) AS RetentionPct\n  FROM RepeatInfo\n  WHERE MonthsSinceFirst &gt; 0\n  GROUP BY MonthsSinceFirst\n  -- Arrange by \"per month offset\" (Low to High)\n  ORDER BY MonthsSinceFirst;\"\"\"\n  ).df()\n  \n## Retention Across Cohorts\nres_cohort_df = con_chinook.execute(\n  \"\"\"-- Retention across cohorts\n  -- First purchase per customer and cohort assignment\n  WITH FirstPurchases AS (\n      SELECT\n          CustomerId,\n          MIN(DATE(InvoiceDate)) AS FirstPurchaseDate,\n          DATE_TRUNC('month', MIN(InvoiceDate)) AS CohortMonth\n      FROM Invoice\n      GROUP BY CustomerId\n  ),\n  \n  -- All purchases labeled with cohort info\n  AllPurchases AS (\n      SELECT\n          i.CustomerId,\n          DATE(i.InvoiceDate) AS PurchaseDate,\n          DATE_TRUNC('month', i.InvoiceDate) AS PurchaseMonth,\n          fp.CohortMonth,\n          DATE_DIFF('month', fp.FirstPurchaseDate, i.InvoiceDate) AS MonthsSinceFirst\n      FROM Invoice i\n      JOIN FirstPurchases fp ON i.CustomerId = fp.CustomerId\n  ),\n  \n  -- Cohort sizes\n  CohortSizes AS (\n      SELECT\n          CohortMonth,\n          COUNT(DISTINCT CustomerId) AS CohortSize\n      FROM FirstPurchases\n      GROUP BY CohortMonth\n  ),\n  \n  -- Distinct customer activity per cohort/month\n  CustomerActivity AS (\n      SELECT DISTINCT\n          CustomerId,\n          CohortMonth,\n          MonthsSinceFirst\n      FROM AllPurchases\n  )\n  \n  -- Final aggregation with cohort size and retention %\n  SELECT\n      ca.CohortMonth,\n      ca.MonthsSinceFirst,\n      COUNT(DISTINCT ca.CustomerId) AS NumActiveCustomers,\n      cs.CohortSize,\n      ROUND(100.0 * COUNT(DISTINCT ca.CustomerId)::DOUBLE / cs.CohortSize, 1) AS RetentionPct\n  FROM CustomerActivity ca\n  JOIN CohortSizes cs ON ca.CohortMonth = cs.CohortMonth\n  GROUP BY ca.CohortMonth, ca.MonthsSinceFirst, cs.CohortSize\n  ORDER BY ca.CohortMonth, ca.MonthsSinceFirst;\"\"\"\n  ).df()\n\n\n\n\n\n\nRetention Decay Plot\nTo better understand customer retention over time, a retention decay curve was generated.\n\nRPython\n\n\n\n\nShow Code\n# Retention Decay Plot\n# Add a column for formatted hover text\nres_repeat_df$hover_text &lt;- paste0(\n  \"Month: \", res_repeat_df$MonthsSinceFirst, \"\\n\",\n  \"Retention: \", res_repeat_df$RetentionPct, \"%\"\n)\n\n# Retention Curve Line Plot\np &lt;- ggplot2::ggplot(\n  res_repeat_df, \n  ggplot2::aes(x = MonthsSinceFirst, y = RetentionPct)) +\n  # Make line plot - emphasize points\n  ggplot2::geom_line(color = \"cadetblue4\", size = 1.2) +\n  ggplot2::geom_point(color = \"cadetblue4\", size = 2, ggplot2::aes(text = hover_text)) +\n  # Mark every 3 months on the x axis\n  ggplot2::scale_x_continuous(\n    breaks = seq(0, max(res_repeat_df$MonthsSinceFirst), by = 3)\n  ) +\n  # Label y axis with \"%\"\n  scale_y_continuous(labels = function(x) paste0(x, \"%\"), limits = c(0, 100)) +\n  # Add labels\n  ggplot2::labs(\n    title = \"Customer Retention Decay &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n    x = \"Months Since First Purchase\",\n    y = \"Retention (%)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n# Convert to interactive plotly for quarto display\nplotly::ggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Retention Decay Plot\n# Add a column for formatted hover text\nres_repeat_df[\"hover_text\"] = (\n    \"Month: \" + res_repeat_df[\"MonthsSinceFirst\"].astype(str) + \"&lt;br&gt;\" +\n    \"Retention: \" + res_repeat_df[\"RetentionPct\"].astype(str) + \"%\"\n)\n\n# Use a function to suppress intermediate outputs from quarto rendering\ndef build_plot(df):\n\n  # Initialize the plot\n  fig = go.Figure()\n\n  # Retention Curve Line Plot\n  fig.add_trace(go.Scatter(\n      x=res_repeat_df[\"MonthsSinceFirst\"],\n      y=res_repeat_df[\"RetentionPct\"],\n      mode=\"lines+markers\",\n      line=dict(color=\"cadetblue\", width=2),\n      marker=dict(size=6),\n      hovertext=res_repeat_df[\"hover_text\"],\n      hoverinfo=\"text\",\n      name=\"Retention\"\n  ))\n\n  # Labels\n  fig.update_layout(\n      title=dict(\n          text=\"Customer Retention Decay &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n          x=0.5,\n          xanchor=\"center\",\n          font=dict(size=20)\n      ),\n      xaxis=dict(\n          title=\"Months Since First Purchase\",\n          tickmode=\"linear\",\n          tick0=0,\n          dtick=3\n      ),\n      yaxis=dict(\n          title=\"Retention (%)\",\n          range=[0, 100],\n          ticksuffix=\"%\"\n      ),\n      template=\"plotly_white\",\n      height=500\n  )\n\n  # return figure\n  return fig\n\n# Display plot\nfig = build_plot(res_repeat_df)\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nRetention decays quickly after the first 6 months, indicating limited long-term engagement. appears to be sporadic, but largely flat.\nThe pattern is sporadic and largely flat, suggesting irregular purchase cycles or infrequent triggers for re-engagement.\nUnexpected retention spikes between months 28-33 (peaking at 61% in month 31) may reflect anomalies in the synthetic data set, delayed reactivation behavior, or an unknown promotional triggers.\nAfter month 42, retention levels flatten at or below 20%, indicating minimal long-term customer activity.\n\nOpportunities:\n\nImplement re-engagement campaigns at the 4-6 months mark, when drop-off risk is highest.\nIntroduce loyalty programs or time-based offers to reward continued engagement in the first year.\nInvestigate drivers behind the month 28-31 spike. If this behavior was tied to promotions or catalog changes, similar tactics could be used to reactivate dormant customers.\n\n\n\n\n\nRetention Across Cohorts: Heatmap\nFor further perspective of customer retention dynamics, a retention heatmap was generated.\n\nRPython\n\n\n\n\nShow Code\n# Cohort Retention Heatmap\n# Fill in missing Cohort-Month combinations with missing values (`NA`)\nres_cohort_df &lt;- tidyr::complete(\n  res_cohort_df,\n  CohortMonth = seq.Date(\n    min(res_cohort_df$CohortMonth), \n    max(res_cohort_df$CohortMonth), \n    by = \"month\"\n    ),\n  MonthsSinceFirst = seq(\n    min(res_cohort_df$MonthsSinceFirst), \n    max(res_cohort_df$MonthsSinceFirst), \n    by = 1\n    )\n  ) |&gt;\n# Filter out any non-included values (Cohort pre Jan 2009, MonthsSince &lt;= 0)\n  dplyr::filter(\n    MonthsSinceFirst &gt; 0 & CohortMonth &gt;= \"2009-01-01\"\n  )\n\n# Add a column for formatted hover text\nres_cohort_df &lt;- res_cohort_df |&gt;\n  dplyr::mutate(\n    hover_text = ifelse(\n      !is.na(RetentionPct), \n      paste0(\n        \"Cohort: \", format(CohortMonth, \"%b %Y\"), \"\\n\",\n        \"Cohort Size: \", CohortSize, \"\\n\",\n        \"Month: \", MonthsSinceFirst, \"\\n\",\n        \"Retention: \", sprintf(\"%1.2f%%\", RetentionPct)\n        ),\n      paste0(\n        \"Cohort: \", format(CohortMonth, \"%b %Y\"), \"\\n\",\n        \"Month: \", MonthsSinceFirst, \"\\n\",\n        \"No data\"\n        )\n      )\n    )\n\n# Retention Heatmap\np &lt;- ggplot2::ggplot(\n  res_cohort_df, \n  ggplot2::aes(\n    x = MonthsSinceFirst, \n    y = CohortMonth, \n    fill = RetentionPct,\n    text = hover_text\n    )\n  ) +\n  # Put a white box around each tile\n  geom_tile(color = \"white\", size = 0.2) +\n  # Use Color-blind friendly color scheme\n  ggplot2::scale_fill_viridis_c(\n    # Make empty/missing tiles a gray color - does not transfer to plotly\n    na.value = \"gray90\",      \n    # Default version\n    option = \"D\",                \n    # Make higher values a darker color\n    direction = -1               \n  ) +\n  # Make x axis evenly marked \n  ggplot2::scale_x_continuous(breaks = pretty(res_cohort_df$MonthsSinceFirst)) +\n  # Label Y axis in human legible format (Month short name + YYYY)\n  ggplot2::scale_y_date(date_breaks = \"1 month\", date_labels = \"%b %Y\") +\n  # Labels\n  ggplot2::labs(\n    title = \"Customer Retention by Cohort &lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n    x = \"Months Since First Purchase\",\n    y = \"Cohort Month (First Purchase)\",\n    fill = \"Retention (%)\"\n  ) +\n  # Optional styling\n  ggplot2::theme_minimal(base_size = 14) +\n  ggplot2::theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Convert to interactive plotly for quarto display\nplotly::ggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Cohort Retention Heatmap\n# Create grid that fills in missing Cohort-Month combinations with missing values (`NA`)\ncohort_range = pd.date_range(\n  start = res_cohort_df['CohortMonth'].min(), \n  end = res_cohort_df['CohortMonth'].max(), \n  freq='MS'\n  )\n  \nmonth_range = range(\n  res_cohort_df['MonthsSinceFirst'].min(),\n  res_cohort_df['MonthsSinceFirst'].max() + 1\n  )\n\n# Create complete combination grid\nfull_grid = pd.MultiIndex.from_product(\n    [cohort_range, month_range],\n    names=['CohortMonth', 'MonthsSinceFirst']\n).to_frame(index=False)\n\n## Merge with actual data to fill in missing combinations\nres_df = full_grid.merge(\n  res_cohort_df, \n  on=['CohortMonth', 'MonthsSinceFirst'], \n  how='left'\n  )\n\n# Filter out any non-included values (Cohort pre Jan 2009, MonthsSince &lt;= 0)\nres_df = res_df[\n    (res_df['CohortMonth'] &gt;= pd.to_datetime('2009-01-01')) & \n    (res_df['MonthsSinceFirst'] &gt; 0)\n]\n\n# Create a human-legible version of the cohort month for labels\nres_df['CohortLabel'] = res_df['CohortMonth'].dt.strftime('%b %Y')\n\n# Ensure they stay in order\nres_df['CohortLabel'] = pd.Categorical(\n    res_df['CohortLabel'], \n    ordered=True,\n    categories=sorted(res_df['CohortMonth'].dt.strftime('%b %Y').unique(), key=lambda x: pd.to_datetime(x))\n)\n\n# Add a column for formatted hover text\nres_df['hover_text'] = np.where(\n    ~res_df['RetentionPct'].isna(),\n    \"Cohort: \" + res_df['CohortMonth'].dt.strftime(\"%b %Y\") +\n    \"&lt;br&gt;Cohort Size: \" + res_df['CohortSize'].astype(str) +\n    \"&lt;br&gt;Month: \" + res_df['MonthsSinceFirst'].astype(str) +\n    \"&lt;br&gt;Retention: \" + (res_df['RetentionPct']).round(2).astype(str) + \"%\",\n    \"Cohort: \" + res_df['CohortLabel'].astype(str) +\n    \"&lt;br&gt;Month: \" + res_df['MonthsSinceFirst'].astype(str) +\n    \"&lt;br&gt;No data\"\n)\n\n# Use a function to suppress intermediate outputs from quarto rendering\ndef build_plot(df):\n  # Create pivot tables\n  retention = df.pivot(index=\"CohortLabel\", columns=\"MonthsSinceFirst\", values=\"RetentionPct\")\n  text = df.pivot(index=\"CohortLabel\", columns=\"MonthsSinceFirst\", values=\"hover_text\")\n\n  # Create heatmap with Plotly\n  fig = go.Figure(\n      data=go.Heatmap(\n          z=retention.values,\n          x=retention.columns,\n          y=retention.index,\n          # Set hover-text\n          text=text.values,\n          hoverinfo='text',\n          # Colorblind-friendly color scheme (Reverse - darker is higher %)\n          colorscale='Viridis_r',\n          zmin=0,\n          zmax=100,\n          colorbar=dict(title=\"Retention (%)\"),\n          showscale=True,\n          hoverongaps=False,\n          zauto=False\n      )\n  )\n\n  # Layout (Labels)\n  fig.update_layout(\n      title=\"Customer Retention by Cohort&lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;\",\n      xaxis_title=\"Months Since First Purchase\",\n      yaxis_title=\"Cohort Month (First Purchase)\",\n      xaxis=dict(type='category'),\n      yaxis=dict(type='category'),\n      height=600\n  )\n  \n  # return figure\n  return fig\n\n# Display plot\nfig = build_plot(res_df)\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThere are no strong diagnoal trends, suggesting that retention behavior is not driven by seasonality or time-of-year events.\nVertical spikes at months 1, 3, 6, 9, 14, 28, 31, 33, 40, and 42 indicate that certain months consistently see higher re-engagement, regardless of cohort. These patterns are more pronounced in later cohorts (September 2009 and onward).\nEarlier cohorts (Jan - Aug 2009) show more scattered retention spikes in later months (e.g, 18-27, 46-53), though these are generally smaller in magnitude.\nThe strongest retention occurs at month 31, with most cohorts retaining 50% or more of their users.\nThe absence of a consistent decay slope and the presence of periodic spikes suggest irregular customer behavior, potentially influenced by catalog changes, promotions, or synthetic data quirks.\n\nOpportunities:\n\nInvestigate month 31 and other spike periods for possible triggers. Catalog expansions, price changes, or customer lifecycle milestones could be replicated or scaled.\nDesign targeted campaigns around known high-retention intervals (e.g., months 1, 3, 6, 9) to encourage early and repeat engagement.\nConsider dynamic lifecycle-based messaging rather than time-based marketing, given the erratic nature of long-term retention.\nExplore subscription models or bundling strategies to stabilize and extend engagement beyond the initial 6–9 month window."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#q3-summary",
    "href": "projects/2025_bi_chinook.html#q3-summary",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Summary of Exploratory Findings",
    "text": "Summary of Exploratory Findings\nCustomer retention drops steeply within the first 6 months after initial purchase, with retention falling below 20% beyond month 42. This indicates that long-term engagement is limited, and most customers do not return frequently after their initial purchases. Unexpected spikes in retention at months 28–33 suggest irregular purchase cycles or promotional triggers. These spikes are consistent across cohorts, especially in later cohorts after September 2009.\n\nPeak retention at month 31.\nInitial drop-off after month 6.\n\n\n\n\n\n\n\nPossible Next Steps\n\n\n\nIf this were real-world data, the following actions could strengthen both analytical insight and strategic decision-making.\nDeepen the Analysis:\n\nInvestigate the causes of retention spikes, particularly around month 31, through detailed promotional or catalog event logs.\nAnalyze customer segments to identify differences in retention behavior and tailor marketing strategies accordingly.\nExplore product-level repeat purchase patterns for granular insights.\n\nStrategic Business Opportunities:\n\nImplement re-engagement campaigns at the 4-6 months mark, when drop-off risk is highest.\nIntroduce loyalty programs or time-based offers to reward continued engagement in the first year.\nTrack customer reactivation triggers more systematically to reduce sporadic behavior and increase predictable retention patterns."
  },
  {
    "objectID": "projects/2025_bi_chinook.html#possible-next-steps-3",
    "href": "projects/2025_bi_chinook.html#possible-next-steps-3",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Possible Next Steps",
    "text": "Possible Next Steps\nIf this were real-world data, the following actions could strengthen both analytical insight and strategic decision-making.\n\nDeepen the Analysis\n\nInvestigate Australia’s 2012–13 revenue decline to identify churn drivers or external influences.\nAnalyze U.S. customer cohorts for upsell and retention opportunities given its dominant revenue role.\nPerform attribute-level analysis within top genres to uncover key drivers of efficiency.\nExamine cross-genre customer behavior and its impact on lifetime value.\nExplore causes behind retention spikes and identify effective promotional triggers.\nSegment customers by retention behavior for tailored marketing.\nStudy product-level repeat purchases for granular insights.\n\n\n\nStrategic Business Opportunities\n\nTarget acquisition in underpenetrated but high-value markets like Chile, Hungary, and Ireland.\nLocalize content and marketing for emerging regions such as India and broader Asia.\nRe-engage lapsed users in markets with past revenue declines (e.g., Brazil, Sweden).\nExpand premium, episodic content in high-return genres like Sci Fi and Comedy.\nCapitalize on temporal demand spikes with curated playlists and limited-time offers.\nOptimize large but underperforming catalogs (e.g., Rock) through marketing or pruning.\nMaintain content momentum in high-penetration genres by refreshing catalogs.\nImplement targeted re-engagement campaigns at 4-6 months to counter early churn.\nDevelop loyalty programs and systematically track reactivation triggers to enhance retention."
  }
]