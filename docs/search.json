[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome! Here you’ll find a small collection of personal projects that reflect some of my skills, interests, and growth. Feel free to explore — use the filters to browse by category and dive into the areas that interest you the most.\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)\n\n\n\nSQL\n\nR\n\nPython\n\nDashboard\n\nData Analysis\n\nBusiness Intelligence\n\nExploratory Analysis\n\nRetail Analytics\n\nChinook\n\nShort\n\nDuckDB\n\n\n\nThis project explores the Chinook dataset — a mock digital music store — to uncover key business insights around revenue, customers, and performance. It combines SQL analysis with dashboard development to present findings visually.\n\n\n\n\n\nJun 30, 2025\n\n\nMorrigan M.\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m mainly excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I was a critical care nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner.\n\n\n\n\nMPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology\n\n\n\n\n\nGraduate Research Assistant, February 2023 - Present\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\nCritical Care Travel Nurse, March 2021 - December 2022\nTravel Nurse Across America (USA)\n\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\nRegistered Nurse (Residency), March 2020 - March 2021\nRochester General Hospital (Rochester, NY, USA)\nPublic Safety Communications Specialist, February 2018 - February 2020\nCity of Plano (Plano, TX, USA)"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m mainly excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I was a critical care nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Morri",
    "section": "",
    "text": "MPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Morri",
    "section": "",
    "text": "Graduate Research Assistant, February 2023 - Present\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\nCritical Care Travel Nurse, March 2021 - December 2022\nTravel Nurse Across America (USA)\n\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\nRegistered Nurse (Residency), March 2020 - March 2021\nRochester General Hospital (Rochester, NY, USA)\nPublic Safety Communications Specialist, February 2018 - February 2020\nCity of Plano (Plano, TX, USA)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m especially excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I spent over a decade as a nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner. (Also: the dog’s name is Lucy. She does not code.)\n\n\n\n\nMPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology\n\n\n\n\n\n\nUTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\n\n\n\nTravel Nurse Across America, USA\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\n\n\n\nRochester General Hospital, Rochester, NY, USA\n\n\n\nCity of Plano, Plano, TX, USA"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About Morri",
    "section": "",
    "text": "Hi — I’m Morri, a data scientist with a clinical background, an MPH in progress, and a serious love of health data. I work at the intersection of epidemiology, bioinformatics, and analytics, building tools that make large, messy datasets more useful for research and public health. I’m especially excited by omics, reproducibility, and clever ways to reduce human effort without sacrificing rigor.\nBefore pivoting to data science, I spent over a decade as a nurse and educator — so I know my way around both a CRRT machine and a good data pipeline. My projects range from fuzzy-matching patient records across data sets, to meta-analyzing EWAS studies, to modeling viral phylogenies for vaccine strain selection.\nI care deeply about clarity, collaboration, and doing science that’s useful. I’d love to work on research teams, data-heavy health projects, or anything that needs solid analysis and a thoughtful partner. (Also: the dog’s name is Lucy. She does not code.)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Morri",
    "section": "",
    "text": "MPH in Epidemiology, In Progress (Est. December 2025)\nUniversity of Texas Health Sciences Center, School of Public Health (Houston, TX, USA)\nCertificate: Genomics & Bioinformatics\nCertificate: Data Science\nThesis: Epigenome-Wide Associations of Factor VIII and Von Willebrand Factor Levels (In Progress, Meta-Analysis, de Vries Lab)\nBSc in Nursing, 2019\nUniversity of Texas at Arlington (Arlington, TX, USA)\nBSc in Biology, 2013\nUniversity of Texas at Dallas (Richardson, TX, USA)  Minor: Sociology"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About Morri",
    "section": "",
    "text": "UTHealth Sciences Center, School of Public Health (Houston, TX, USA)\n\nPI: Dr. M. Brad Cannell\nContributed to public health research projects (DETECT, DETECT-RPC, Link2Care), supporting the full data lifecycle — from collection and cleaning to analysis and reporting.\nDeveloped and implemented probabilistic fuzzy-matching techniques using the fastLink R package to link subject data across datasets lacking common identifiers.\nDrafted statistical analysis plans, IRB protocols, informed consent forms, and conducted statistical analyses in R.\n\n\n\n\nTravel Nurse Across America, USA\n\nRapidly deployed to hospitals nationwide, delivering expert-level critical care in ICU and ED settings, managing complex cases across multiple specialties.\nLed interdisciplinary teams, acted as Charge Nurse and educator, and routinely utilized advanced technologies (e.g. ECMO, IABP, CRRT, Impella, and Blakemore/Compass, etc.)\n\n\n\n\nRochester General Hospital, Rochester, NY, USA\n\n\n\nCity of Plano, Plano, TX, USA"
  },
  {
    "objectID": "projects/draft.html",
    "href": "projects/draft.html",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "Goal: Build a BI dashboard for a fictional digital media business.\nFocus: Revenue insights, customer analysis, business performance.\nWhy it matters: Business stakeholders benefit from fast, accessible reporting on key performance indicators (KPIs) to support decision-making.\n\nThis project is intended for both technical and business audiences, with clear visual insights supported by SQL-based exploration.\nFor the best visualization experience, browser is recommended - optimization of these exploratory visualizations is limited.\n\n\nThis project is designed to help answer the following business questions:\n\nWhere is revenue coming from geographically?\nWhat genres or artists generate the most income?\nHow many customers are repeat buyers?\nHow do sales trends evolve over time?\n\n\n\n\n\nSQL-based analysis of key business metrics with visualizations in both R and Python.\nDashboards built in R (Shiny) and Python (Dash) [TO DO].\n[Considering a tableau version of the dashboard; TO DO]\n\n\n\n\nThe following tools were used to merge, analyze, and present the data:\n\n\n\nTool\nPurpose\n\n\n\n\nSQL\nData transformation and KPIs\n\n\nDuckDB\nLightweight, embedded relational database\n\n\nR + Shiny\nDashboard development (R-based)\n\n\nPython + Dash\nDashboard development (Python-based)\n\n\n\nThis project demonstrates dashboard development in both R (Shiny) and Python (Dash), highlighting flexibility across technical ecosystems.\n[NOTE TO SELF - DELETE ONCE REVISED: This is a work in progress. I plan to make a dashboard with R and one with Python, just to show I can do both. I will update this to reflect the final project once it’s done.]\n\nRPython\n\n\n\n\nShow Code\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(DBI)\nlibrary(plotly)\nlibrary(countrycode)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\n\n\n\nShow Code\nimport duckdb\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.graph_objs import Figure\nimport pycountry"
  },
  {
    "objectID": "projects/draft.html#business-questions",
    "href": "projects/draft.html#business-questions",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "This project is designed to help answer the following business questions:\n\nWhere is revenue coming from geographically?\nWhat genres or artists generate the most income?\nHow many customers are repeat buyers?\nHow do sales trends evolve over time?"
  },
  {
    "objectID": "projects/draft.html#deliverables",
    "href": "projects/draft.html#deliverables",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "SQL-based analysis of key business metrics with visualizations in both R and Python.\nDashboards built in R (Shiny) and Python (Dash) [TO DO].\n[Considering a tableau version of the dashboard; TO DO]"
  },
  {
    "objectID": "projects/draft.html#tech-stack",
    "href": "projects/draft.html#tech-stack",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "",
    "text": "The following tools were used to merge, analyze, and present the data:\n\n\n\nTool\nPurpose\n\n\n\n\nSQL\nData transformation and KPIs\n\n\nDuckDB\nLightweight, embedded relational database\n\n\nR + Shiny\nDashboard development (R-based)\n\n\nPython + Dash\nDashboard development (Python-based)\n\n\n\nThis project demonstrates dashboard development in both R (Shiny) and Python (Dash), highlighting flexibility across technical ecosystems.\n[NOTE TO SELF - DELETE ONCE REVISED: This is a work in progress. I plan to make a dashboard with R and one with Python, just to show I can do both. I will update this to reflect the final project once it’s done.]\n\nRPython\n\n\n\n\nShow Code\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(DBI)\nlibrary(plotly)\nlibrary(countrycode)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\n\n\n\nShow Code\nimport duckdb\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.graph_objs import Figure\nimport pycountry"
  },
  {
    "objectID": "projects/draft.html#connection-validation",
    "href": "projects/draft.html#connection-validation",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Connection and Validation",
    "text": "Connection and Validation\nA connection was made to the DuckDB database file.\n\nRPython\n\n\n\n\nShow Code\ncon_chinook &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"../data/chinook.duckdb\",\n  read_only = TRUE\n  )\n\n\n\n\n\n\nShow Code\ncon_chinook = duckdb.connect(\"../data/chinook.duckdb\", read_only = True)\n\n\n\n\n\nInitial exploration confirmed the data’s structure and date range matched documentation.\nA query of date values in the InvoiceDate table confirmed that the data contained records with a date range from 2009-01-01 to 2013-12-22.\n\n\n\nShow Code\n\nSQL\n\n-- Get date range of Invoices\nSELECT \n  MIN(i.InvoiceDate) as MinDate, \n  MAX(i.InvoiceDate) as MaxDate\nFROM Invoice i;\n\n\n\n\n1 records\n\n\nMinDate\nMaxDate\n\n\n\n\n2009-01-01\n2013-12-22\n\n\n\n\n\nAs expected, InvoiceLine and Track had the highest number of unique records, reflecting their one-to-many relationships with Invoice and Album, respectively. Metadata tables such as Genre and MediaType had fewer unique values.\n\n\n\nShow Code\n\nSQL\n\n-- Get Number of Unique Key Values in Each Table\nSELECT \n  'Employees' AS TableName, \n  COUNT(DISTINCT EmployeeId) AS UniqueKeys \nFROM Employee\nUNION ALL\nSELECT \n  'Customers' AS TableName, \n  COUNT(DISTINCT Customerid) AS UniqueKeys \nFROM Customer\nUNION ALL\nSELECT \n  'Invoices' AS TableName, \n  COUNT(DISTINCT InvoiceId) AS UniqueKeys \nFROM Invoice\nUNION ALL\nSELECT \n  'Invoice Lines' AS TableName, \n  COUNT(DISTINCT InvoiceLineId) AS UniqueKeys \nFROM InvoiceLine\nUNION ALL\nSELECT \n  'Tracks' AS TableName, \n  COUNT(DISTINCT TrackId) AS UniqueKeys \nFROM Track\nUNION ALL\nSELECT \n  'Artists' AS TableName, \n  COUNT(DISTINCT ArtistId) AS UniqueKeys \nFROM Artist\nUNION ALL\nSELECT \n  'Albums' AS TableName, \n  COUNT(DISTINCT AlbumId) AS UniqueKeys \nFROM Album\nUNION ALL\nSELECT \n  'Genres' AS TableName, \n  COUNT(DISTINCT GenreId) AS UniqueKeys \nFROM Genre\nUNION ALL\nSELECT \n  'Media Types' AS TableName,\n  COUNT(DISTINCT MediaTypeId) AS UniqueKeys \nFROM MediaType\nUNION ALL\nSELECT \n  'Playlists' AS TableName, \n  COUNT(DISTINCT PlaylistId) AS UniqueKeys \nFROM Playlist\nORDER BY UniqueKeys DESC;\n\n\n\n\nDisplaying records 1 - 10\n\n\nTableName\nUniqueKeys\n\n\n\n\nTracks\n3503\n\n\nInvoice Lines\n2240\n\n\nInvoices\n412\n\n\nAlbums\n347\n\n\nArtists\n275\n\n\nCustomers\n59\n\n\nGenres\n25\n\n\nPlaylists\n18\n\n\nEmployees\n8\n\n\nMedia Types\n5\n\n\n\n\n\nNext Step: Key performance indicators (KPIs) will be extracted with SQL and visualized with R and Python to answer the business questions outlined previously, beginning with a geographic revenue analysis."
  },
  {
    "objectID": "projects/draft.html#q1-vis",
    "href": "projects/draft.html#q1-vis",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Exploratory Visualizations",
    "text": "Exploratory Visualizations\nIn preparation for exploratory visualization generation, the data is retrieved using SQL queries and prepared in both R and Python.\n\nRPython\n\n\n\n\nShow Code\n# SQL Queries\n## Yearly Breakdown\nres_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n    -- Get Country and Year for grouping\n    i.BillingCountry as Country, \n    YEAR(i.InvoiceDate) as Year,\n    -- Calculate Total Revenue\n    SUM(i.Total) AS TotalRevenue,\n    -- Calculate % of Total/Global Revenue\n    ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n    -- Get Number of Customers\n    COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n    -- Calculate Revenue per Customer\n    ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\nFROM Customer c\nJOIN Invoice i on c.CustomerId == i.CustomerId\nGROUP BY i.BillingCountry, Year\n-- Sort Revenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n## Total (all years)\nres_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n    -- Get Country for grouping\n    i.BillingCountry as Country,\n    -- Set 'Year' to 'All' for grouping\n    'All' AS Year,\n    -- Calculate Total Revenue\n    SUM(i.Total) AS TotalRevenue,\n    -- Calculate % of Total/Global Revenue\n    ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n    -- Get Number of Customers\n    COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n    -- Calculate Revenue per Customer\n    ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\nFROM Customer c\nJOIN Invoice i on c.CustomerId == i.CustomerId\nGROUP BY i.BillingCountry,\n-- Sort Revenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n# Combine data frames in R\nres_df &lt;- dplyr::bind_rows(\n  res_agg_df,\n  res_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  ) |&gt;\n  dplyr::mutate(\n    ## Add ISO Country Codes\n    iso_alpha = countrycode::countrycode(\n      Country, \n      origin = 'country.name', \n      destination = 'iso3c'\n      ),\n    ## Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $TotalRevenue.##\")\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(TotalRevenue, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get vector of unique years (layers/traces) - order with \"All\" first.\nyears &lt;- c(\"All\", sort(unique(res_yearly_df$Year)))\n\n\n\n\n\n\nShow Code\n# SQL Queries\n## Yearly Breakdown\nres_yearly_df = con_chinook.execute(\n    \"\"\"SELECT \n      -- Get Country and Year for grouping\n      i.BillingCountry as Country, \n      YEAR(i.InvoiceDate) as Year,\n      -- Calculate Total Revenue\n      SUM(i.Total) AS TotalRevenue,\n      -- Calculate % of Total/Global Revenue\n      ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n      -- Get Number of Customers\n      COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n      -- Calculate Revenue per Customer\n      ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\n  FROM Customer c\n  JOIN Invoice i on c.CustomerId == i.CustomerId\n  GROUP BY i.BillingCountry, Year\n  -- Sort Revenue (Highest to Lowest)\n  ORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n\n## Total (all years)\nres_agg_df = con_chinook.execute(\n    \"\"\"SELECT \n      -- Get Country for grouping\n      i.BillingCountry as Country,\n      -- Set 'Year' to 'All' for grouping\n      'All' AS Year,\n      -- Calculate Total Revenue\n      SUM(i.Total) AS TotalRevenue,\n      -- Calculate % of Total/Global Revenue\n      ROUND(SUM(i.Total)*100.0 / (SELECT SUM(Total) from Invoice), 2) AS PercentGlobalRevenue,\n      -- Get Number of Customers\n      COUNT(DISTINCT c.CustomerId) AS NumCustomers,\n      -- Calculate Revenue per Customer\n      ROUND(SUM(i.Total) / COUNT(DISTINCT c.CustomerId), 2) AS RevenuePerCustomer\n  FROM Customer c\n  JOIN Invoice i on c.CustomerId == i.CustomerId\n  GROUP BY i.BillingCountry,\n  -- Sort Revenue (Highest to Lowest)\n  ORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n\n# Combine data frames and ensure consistent types\nres_df = pd.concat([\n  res_agg_df,\n  res_yearly_df.assign(Year=res_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n\n# Add ISO Country Codes\ndef get_iso_alpha3(country_name):\n try:\n   match = pycountry.countries.search_fuzzy(country_name)\n   return match[0].alpha_3\n except LookupError:\n   return None\n \nres_df['iso_alpha'] = res_df['Country'].apply(get_iso_alpha3)\n\n# Get unique years (layers/traces) - order with \"All\" first.\nyears = [\"All\"] + sorted(res_df[res_df['Year'] != 'All']['Year'].unique().tolist())\n\n\n\n\n\n\nTotal Revenue: Choropleth by Country\nTotal revenue per country across the entire data set was visualized with a Choropeth plot.\n\nRPython\n\n\n\n\nShow Code\n# Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $TotalRevenue.##\")\nres_df &lt;- res_df |&gt; \n  dplyr::mutate(\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(TotalRevenue, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get minimum and maximum values for TotalRevenue (Colorbar consistency)\nz_min_val &lt;- min(res_df$TotalRevenue, na.rm = TRUE)\nz_max_val &lt;- max(res_df$TotalRevenue, na.rm = TRUE)\n\n# Generate plotly Choropleth\nfig &lt;- plotly::plot_ly(\n  data = res_df,\n  type = 'choropleth',\n  locations = ~iso_alpha,\n  z = ~TotalRevenue,\n  # Set hover text to only display our desired, formatted output\n  text = ~hover_text,\n  hoverinfo = \"text\",\n  frame = ~Year,\n  # Set minimum and maximum TotalRevenue values, for consistent scale\n  zmin = z_min_val,\n  zmax = z_max_val,\n  # Title Colorbar/Legend\n  colorbar = list(\n    title = \"Total Revenue (USD$)\"\n  ),\n  # Color-blind friendly color scale\n  colorscale = \"Viridis\",\n  reversescale = TRUE,\n  showscale = TRUE,\n  # Give national boundaries a dark gray outline\n  marker = list(line = list(color = \"darkgrey\", width = 0.5))\n)\n\n# Layout with animation controls\nfig &lt;- fig %&gt;%\n  plotly::layout(\n    title = list(\n      text = \"Total Revenue by Country &lt;br&gt; 2009-01-01 to 2013-12-22\",\n      x = 0.5,\n      xanchor = \"center\",\n      font = list(size = 18)\n    ),\n    geo = list(\n      # Add a neat little frame around the world\n      showframe = TRUE, \n      # Add coast lines - ensures countries that aren't in data are seen\n      showcoastlines = TRUE, \n      # Use natural earth projection\n      projection = list(type = 'natural earth')\n      ),\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        showactive = TRUE,\n        buttons = purrr::map(years, function(yr) {\n          list(\n            method = \"animate\",\n            args = list(list(yr), list(mode = \"immediate\", frame = list(duration = 0, redraw = TRUE))),\n            label = yr\n          )\n        }),\n        # Positioning of dropdown menu\n        x = 0.1,\n        y = 1.15,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    ),\n    margin = list(t = 80)\n  ) %&gt;%\n  plotly::animation_opts(frame = 1000, transition = 0, redraw = TRUE)\n\n# Display interactive plot\nfig\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Specify hover text\nres_df['hover_text'] = res_df.apply( \\\n  lambda row: f\"&lt;b&gt;{row['Country']}&lt;/b&gt;&lt;br&gt; ${row['TotalRevenue']:.2f}\", axis = 1 \\\n  )\n\n# Get maximum and minimum TotalRevenue values (consistent scale)\nz_min_val = res_df['TotalRevenue'].min()\nz_max_val = res_df['TotalRevenue'].max()\n\n# Create frames (one per year, and aggregate)\nframes = []\n\nfor year in years:\n  df_year = res_df[res_df['Year'] == year]\n  frames.append(go.Frame(\n    name = str(year),\n    data = [go.Choropleth(\n      locations = df_year['iso_alpha'],\n      z = df_year['TotalRevenue'],\n      zmin = z_min_val,\n      zmax = z_max_val,\n      text = df_year['hover_text'],\n      hoverinfo = 'text',\n      # Color-blind friendly color scale (reversed: darker with higher revenues)\n      colorscale = 'Viridis_r',\n      # Give national boundaries a dark grey outline\n      marker = dict(line=dict(color='darkgrey', width=0.5))\n    )]\n  ))\n  \n# First frame (initial state)\ninit_df = res_df[res_df['Year'] == 'All']\n\n# Generate plotly Choropleth\nfig = go.Figure(\n  data=[go.Choropleth(\n        locations=init_df['iso_alpha'],\n        z=init_df['TotalRevenue'],\n        text=init_df['hover_text'],\n        hoverinfo='text',\n        # Color-blind friendly color scale (reversed: darker with higher revenues)\n        colorscale='Viridis_r',\n        zmin=z_min_val,\n        zmax=z_max_val,\n        # Give national boundaries a dark grey outline\n        marker=dict(line=dict(color='darkgrey', width=0.5)),\n        # Title Colorbar/Legend\n        colorbar=dict(title='Total Revenue (USD$)')\n    )],\n    frames=frames\n  )\n  \n# Format Layout with Animation Controls\nfig.update_layout(\n  title = dict(\n    text = \"Total Revenue by Country &lt;br&gt; 2009-01-01 to 2013-12-22\",\n    x = 0.5,\n    xanchor = 'center',\n    font = dict(size=18)\n  ),\n  margin=dict(t=80),\n  # Frame and view\n  geo = dict(\n    # Show countries and boundaries\n    showcountries = True,\n    # Give national boundaries a dark gray outline\n    countrycolor=\"darkgrey\",\n    # Add coast lines - ensure countries that aren't in data are seen\n    showcoastlines = True,\n    coastlinecolor = \"gray\",\n    #  Ad a neat little frame around the world\n    showframe = True,\n    framecolor = \"black\",\n    # Use natural earth projection\n    projection_type = \"natural earth\"\n  ),\n  # Buttons/Menus\n  updatemenus = [dict(\n    ## Play/Pause\n        # First button active by default (yr == \"All\")\n        type = \"buttons\",\n        direction = \"left\",\n        x = 0,\n        y = 0,\n        showactive = False,\n        xanchor = \"left\",\n        yanchor = \"bottom\",\n        pad = dict(r = 10, t = 70),\n        buttons = [dict(\n          label = \"Play\",\n          method = \"animate\",\n          args = [None, {\n            \"frame\": {\"duration\": 1000, \"redraw\": True},\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}\n          }]\n        ), dict(\n          label = \"Pause\",\n          method = \"animate\",\n          args=[[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}] \n          )] \n      )] +\n  ## Year Dropdown Menu\n    [dict(\n      type=\"dropdown\",\n      x = 0.1,\n      y = 1.15,\n      xanchor=\"left\",\n      yanchor=\"top\",\n      showactive=True,\n      buttons=[dict(\n        label=str(year),\n        method=\"animate\",\n        args=[\n            [str(year)],\n            {\"mode\": \"immediate\",\n             \"frame\": {\"duration\": 0, \"redraw\": True},\n             \"transition\": {\"duration\": 0}}\n        ]\n    ) for year in years]\n  )],\n  sliders = [dict(\n      active = 0,\n      # Positioning of slider menu\n      x = 0.1,\n      y = -0.2,\n      len = 0.8,\n      xanchor = \"left\",\n      yanchor = \"bottom\",\n      pad = dict(t= 30, b=10),\n      currentvalue = dict(\n        visible = True,\n        prefix = \"Year: \",\n        xanchor = \"right\",\n        font = dict(size=14, color = \"#666\")\n      ),\n    steps = [dict(\n      method = 'animate',\n      args =[[str(year)], {\n        \"mode\": \"immediate\",\n        \"frame\": {\"duration\": 1000, \"redraw\": True},\n        \"transition\": {\"duration\": 300}\n        }],\n        label = str(year) \n      ) for year in years]\n    )] \n  );\n\n# Display interactive plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe customer base exhibits persistent geographic disparities in both scale and engagement.\n\nOverall revenue trends are relatively flat, with limited signs of organic growth.\nNorth America and Brazil consistently drive the highest revenues, indicating strong market presence and sustained demand.\nIndia and parts of Central Europe maintain modest but consistent revenue, suggesting a stable (though modest) consumer base that may respond well to targeted growth strategies.\nAustralia showed signs of growth until 2013, after which revenue dropped to zero — this may reflect market exit, operational changes, or demand saturation.\nOther parts of South America and Europe show sporadic revenue, possibly tied to one-time purchases or minimal customer engagement.\n\nOpportunities:\n\nThere is untapped potential in underrepresented regions. If market demand can be properly assessed and activated — through localized marketing, partnerships, or product adaptation — these regions could represent growth markets.\nSouth America and Europe may benefit from customer acquisition and retention strategies.\nAustralia’s sharp revenue drop in 2013 warrants deeper investigation — identifying root causes could help preempt similar risks in other markets.\n\n\n\n\n\nRevenue per Customer: Choropleth by Country\nInitial exploration revealed a significant mismatch between total revenue and revenue per customer. This was further explored by forming a similar Choropeth plot.\n\nRPython\n\n\n\n\nShow Code\n# Format Hover Text (&lt;b&gt;Country:&lt;/b&gt;&lt;br&gt; $RevenuePerCustomer.##\")\nres_df &lt;- res_df |&gt; \n  dplyr::mutate(\n    hover_text = paste0(\n      \"&lt;b&gt;\", Country, \":&lt;/b&gt;&lt;br&gt; $\",\n      formatC(RevenuePerCustomer, format = 'f', big.mark =\",'\", digits = 2)\n      )\n    ) \n\n# Get minimum and maximum values for RevenuePerCustomer (Colorbar consistency)\nz_min_val &lt;- min(res_df$RevenuePerCustomer, na.rm = TRUE)\nz_max_val &lt;- max(res_df$RevenuePerCustomer, na.rm = TRUE)\n\n# Generate plotly Choropleth\nfig &lt;- plotly::plot_ly(\n  data = res_df,\n  type = 'choropleth',\n  locations = ~iso_alpha,\n  z = ~RevenuePerCustomer,\n  # Set hover text to only display our desired, formatted output\n  text = ~hover_text,\n  hoverinfo = \"text\",\n  frame = ~Year,\n  # Set minimum and maximum RevenuePerCustomer values, for consistent scale\n  zmin = z_min_val,\n  zmax = z_max_val,\n  # Title Colorbar/Legend\n  colorbar = list(\n    title = \"Revenue per Customer (USD$)\"\n  ),\n  # Color-blind friendly color scale\n  colorscale = \"Viridis\",\n  reversescale = TRUE,\n  showscale = TRUE,\n  # Give national boundaries a dark gray outline\n  marker = list(line = list(color = \"darkgrey\", width = 0.5))\n)\n\n# Layout with animation controls\nfig &lt;- fig %&gt;%\n  plotly::layout(\n    title = list(\n      text = \"Revenue per Customer by Country &lt;br&gt; 2009-01-01 to 2013-12-22\",\n      x = 0.5,\n      xanchor = \"center\",\n      font = list(size = 18)\n    ),\n    geo = list(\n      # Add a neat little frame around the world\n      showframe = TRUE, \n      # Add coast lines - ensures countries that aren't in data are seen\n      showcoastlines = TRUE, \n      # Use natural earth projection\n      projection = list(type = 'natural earth')\n      ),\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        showactive = TRUE,\n        buttons = purrr::map(years, function(yr) {\n          list(\n            method = \"animate\",\n            args = list(list(yr), list(mode = \"immediate\", frame = list(duration = 0, redraw = TRUE))),\n            label = yr\n          )\n        }),\n        # Positioning of dropdown menu\n        x = 0.1,\n        y = 1.15,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    ),\n    margin = list(t = 80)\n  ) %&gt;%\n  plotly::animation_opts(frame = 1000, transition = 0, redraw = TRUE)\n\n# Display interactive plot\nfig\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Specify hover text\nres_df['hover_text'] = res_df.apply( \\\n  lambda row: f\"&lt;b&gt;{row['Country']}&lt;/b&gt;&lt;br&gt; ${row['RevenuePerCustomer']:.2f}\", axis = 1 \\\n  )\n\n# Get maximum and minimum RevenuePerCustomer values (consistent scale)\nz_min_val = res_df['RevenuePerCustomer'].min()\nz_max_val = res_df['RevenuePerCustomer'].max()\n\n# Create frames (one per year, and aggregate)\nframes = []\n\nfor year in years:\n  df_year = res_df[res_df['Year'] == year]\n  frames.append(go.Frame(\n    name = str(year),\n    data = [go.Choropleth(\n      locations = df_year['iso_alpha'],\n      z = df_year['RevenuePerCustomer'],\n      zmin = z_min_val,\n      zmax = z_max_val,\n      text = df_year['hover_text'],\n      hoverinfo = 'text',\n      # Color-blind friendly color scale (reversed: darker with higher revenues)\n      colorscale = 'Viridis_r',\n      # Give national boundaries a dark grey outline\n      marker = dict(line=dict(color='darkgrey', width=0.5))\n    )]\n  ))\n  \n# First frame (initial state)\ninit_df = res_df[res_df['Year'] == 'All']\n\n# Generate plotly Choropleth\nfig = go.Figure(\n  data=[go.Choropleth(\n        locations=init_df['iso_alpha'],\n        z=init_df['RevenuePerCustomer'],\n        text=init_df['hover_text'],\n        hoverinfo='text',\n        # Color-blind friendly color scale (reversed: darker with higher revenues)\n        colorscale='Viridis_r',\n        zmin=z_min_val,\n        zmax=z_max_val,\n        # Give national boundaries a dark grey outline\n        marker=dict(line=dict(color='darkgrey', width=0.5)),\n        # Title Colorbar/Legend\n        colorbar=dict(title='Revenue per Customer (USD$)')\n    )],\n    frames=frames\n  )\n  \n# Format Layout with Animation Controls\nfig.update_layout(\n  title = dict(\n    text = \"Revenue per Customer by Country &lt;br&gt; 2009-01-01 to 2013-12-22\",\n    x = 0.5,\n    xanchor = 'center',\n    font = dict(size=18)\n  ),\n  margin=dict(t=80),\n  # Frame and view\n  geo = dict(\n    # Show countries and boundaries\n    showcountries = True,\n    # Give national boundaries a dark gray outline\n    countrycolor=\"darkgrey\",\n    # Add coast lines - ensure countries that aren't in data are seen\n    showcoastlines = True,\n    coastlinecolor = \"gray\",\n    #  Ad a neat little frame around the world\n    showframe = True,\n    framecolor = \"black\",\n    # Use natural earth projection\n    projection_type = \"natural earth\"\n  ),\n  # Buttons/Menus\n  updatemenus = [dict(\n    ## Play/Pause\n        # First button active by default (yr == \"All\")\n        type = \"buttons\",\n        direction = \"left\",\n        x = 0,\n        y = 0,\n        showactive = False,\n        xanchor = \"left\",\n        yanchor = \"bottom\",\n        pad = dict(r = 10, t = 70),\n        buttons = [dict(\n          label = \"Play\",\n          method = \"animate\",\n          args = [None, {\n            \"frame\": {\"duration\": 1000, \"redraw\": True},\n            \"fromcurrent\": True,\n            \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}\n          }]\n        ), dict(\n          label = \"Pause\",\n          method = \"animate\",\n          args=[[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}] \n          )] \n      )] +\n  ## Year Dropdown Menu\n    [dict(\n      type=\"dropdown\",\n      x = 0.1,\n      y = 1.15,\n      xanchor=\"left\",\n      yanchor=\"top\",\n      showactive=True,\n      buttons=[dict(\n        label=str(year),\n        method=\"animate\",\n        args=[\n            [str(year)],\n            {\"mode\": \"immediate\",\n             \"frame\": {\"duration\": 0, \"redraw\": True},\n             \"transition\": {\"duration\": 0}}\n        ]\n    ) for year in years]\n  )],\n  sliders = [dict(\n      active = 0,\n      # Positioning of slider menu\n      x = 0.1,\n      y = -0.2,\n      len = 0.8,\n      xanchor = \"left\",\n      yanchor = \"bottom\",\n      pad = dict(t= 30, b=10),\n      currentvalue = dict(\n        visible = True,\n        prefix = \"Year: \",\n        xanchor = \"right\",\n        font = dict(size=14, color = \"#666\")\n      ),\n    steps = [dict(\n      method = 'animate',\n      args =[[str(year)], {\n        \"mode\": \"immediate\",\n        \"frame\": {\"duration\": 1000, \"redraw\": True},\n        \"transition\": {\"duration\": 300}\n        }],\n        label = str(year) \n      ) for year in years]\n    )] \n  );\n\n# Display interactive plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nYear-over-year shifts in revenue per customer suggest evolving engagement patterns across regions.\n\nHigh total-revenue countries (USA, Canada, Brazil) and India show flat revenue per customer trends, indicating stable but possibly casual engagement.\nSouth America saw increases in revenue per customer in both 2010 and 2013, suggesting periods of heightened individual customer value — possibly driven by regional promotions or market-specific trends.\nEurope experienced a spike in 2011, with multiple countries (e.g., Austria, Hungary, Ireland) showing high per-customer revenue. The cause is unclear but may reflect a surge in high-value purchases or regional campaigns\n\nOpportunities:\n\nHigh per-customer revenue regions (especially those with smaller total revenue footprints) may benefit from customer acquisition efforts, as existing users demonstrate strong engagement or purchasing behavior.\nFlat or declining per-customer revenue in large markets highlights a need for upselling, bundling, or personalized offers to increase customer lifetime value.\nInvestigating year-over-year anomalies (e.g., Australia’s 2012 spike, Europe in 2011) could uncover replicable growth levers or emerging market trends."
  },
  {
    "objectID": "projects/draft.html#q1-summary",
    "href": "projects/draft.html#q1-summary",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Summary of Exploratory findings",
    "text": "Summary of Exploratory findings\nTotal revenue measures market size, while revenue per customer reflects intensity of engagement. Both are important for guiding different types of strategic decisions (e.g., acquisition vs. retention).\nGeographic revenue in the Chinook dataset is concentrated in a few strong markets, while many others remain underdeveloped. This disparity presents both risk and opportunity: efforts to deepen engagement in large casual markets (e.g., USA, Brazil) and expand in high-value but small markets (e.g., Austria, Chile) could lead to measurable revenue growth. Year-over-year changes — such as Australia’s exit and Europe’s 2011 spike — suggest that regional trends and operational shifts have meaningful financial impacts worth further investigation.\n\nTop 5 Countries by Total Revenue: USA, Canada, France, Brazil, Germany\nTop 5 Countries by Revenue per Customer: Chile, Hungary, Ireland, Czech Republic, Austria\n\n\n\n\n\n\n\nPossible Next Steps\n\n\n\nIf this were real-world data, the following actions could strengthen both analytical insight and strategic decision-making.\nDeepen the Analysis:\n\nInvestigate Australia’s 2012–13 drop-off.\nExamine customer churn, pricing changes, or external events that may explain the abrupt loss of revenue.\nAnalyze U.S. cohort churn to identify upsell opportunities.\nAs the largest revenue contributor with moderate revenue-per-customer, the U.S. market may contain untapped upsell or retention opportunities.\n\nStrategic Business Opportunities:\n\nLaunch targeted consumer acquisition campaigns in underpenetrated, high-potential markets.\nFocus on Chile, Hungary, the Czech Republic and Irelan — all show strong revenue-per-customer despite smaller customer bases.\nLocalize and invest in regional content for India and Asia.\nThese high-population markets show minimal engagement. Tailored content, pricing, or distribution could unlock substantial growth.\nRe-engage customers in stalled or declining regions.\nCountries like Brazil and Sweden once generated revenue but saw sustained drop-offs. A targeted win-back campaign could reclaim lapsed users."
  },
  {
    "objectID": "projects/draft.html#exploratory-visualizations",
    "href": "projects/draft.html#exploratory-visualizations",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Exploratory Visualizations",
    "text": "Exploratory Visualizations\nIn preparation for exploratory visualization generation, the data is retrieved using SQL queries and prepared in both R and Python.\n\nRPython\n\n\n\n\nShow Code\n# SQL Queries\n## Genre\n### Yearly Breakdown\nres_genre_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook, \n  \"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\")\n\n## Aggregate\nres_genre_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\")\n\n## Artist\nres_artist_yearly_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\nres_artist_agg_df &lt;- DBI::dbGetQuery(\n  con_chinook,\n  \"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\n)\n\n# Combine data frames in R\nres_genre_df &lt;- dplyr::bind_rows(\n  res_genre_agg_df,\n  res_genre_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  )\n\nres_artist_df &lt;- dplyr::bind_rows(\n  res_artist_agg_df,\n  res_artist_yearly_df |&gt; dplyr::mutate(Year = as.character(Year))\n  )\n\n# Get vector of unique years (layers/traces) - order with \"All\" first.\nyears &lt;- c(\"All\", sort(unique(res_genre_yearly_df$Year)))\n\n# Make \"Year\" an ordered factor (Plot generation ordering)\nres_genre_df &lt;- res_genre_df |&gt;\n  dplyr::mutate(Year = factor(Year, levels = years, ordered = TRUE))\n\nres_artist_df &lt;- res_artist_df |&gt;\n  dplyr::mutate(Year = factor(Year, levels = years, ordered = T))\n\n# Get vector of unique Genres - order by total revenue overall\ngenres &lt;- res_genre_agg_df |&gt;\n  dplyr::arrange(dplyr::desc(TotalRevenue)) |&gt;\n  dplyr::select(Genre) |&gt;\n  dplyr::distinct() |&gt; \n  dplyr::pull()\n\n# Make \"Genre\" an ordered factor (Plot generation ordering)\nres_genre_df &lt;- res_genre_df |&gt;\n  dplyr::mutate(\n    Genre = factor(Genre, levels = genres, ordered = T)\n  )\n\n# Get vector of unique Artists - order by total revenue overall\nartists &lt;- res_artist_agg_df |&gt;\n  dplyr::arrange(dplyr::desc(TotalRevenue)) |&gt;\n  dplyr::select(Artist) |&gt;\n  dplyr::distinct() |&gt; \n  dplyr::pull()\n\n# Make \"Genre\" an ordered factor (Plot generation ordering)\nres_artist_df &lt;- res_artist_df |&gt;\n  dplyr::mutate(\n    Genre = factor(Artist, levels = artists, ordered = T)\n  )\n\n\n\n\n\n\nShow Code\n# SQL Queries\n## Genre\n### Yearly Breakdown\nres_genre_yearly_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n### Aggregate\nres_genre_agg_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Get Genre and Year for grouping\n  g.Name AS Genre,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog in this genre\n  track_counts.TotalTracksInGenre,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksInGenre, 2) AS PercentOfTracksSold,\n  -- Revenue per total track in genre\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksInGenre, 2) AS RevenuePerTotalTrack,\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Genre g ON t.GenreId = g.GenreId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT GenreId, COUNT(*) AS TotalTracksInGenre\n    FROM Track\n    GROUP BY GenreId\n) AS track_counts ON g.GenreId = track_counts.GenreId\nGROUP BY g.Name, Year, track_counts.TotalTracksInGenre\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n## Arist\n### Yearly Breakdown\nres_artist_yearly_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  YEAR(i.InvoiceDate) as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Invoice i on il.InvoiceId = i.InvoiceId\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n### Aggregate\nres_artist_agg_df = con_chinook.execute(\n  \"\"\"SELECT \n  -- Select Artist and Year for Grouping\n  ar.Name AS Artist,\n  'All' as Year,\n  -- Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity), 2) AS TotalRevenue,\n  -- Number of Tracks\n  COUNT(*) AS NumTracksSold,\n  -- Average Revenue per Track Sold\n  ROUND(SUM(il.UnitPrice * il.Quantity)/COUNT(*), 2) AS AvgRevenuePerTrack,\n  -- Percentage of Total Revenue\n  ROUND(SUM(il.UnitPrice * il.Quantity)*100.0 / (SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine), 2) AS PercentOfRevenue,\n  -- Percentage of Volume (Units Sold)\n  ROUND(COUNT(*)*100.0 / (SELECT COUNT(*) FROM InvoiceLine),2) AS PercentOfUnitSales,\n  -- Total number of tracks in the catalog for artist\n  track_counts.TotalTracksByArtist,\n  -- Proportion of catalog that was actually sold\n  ROUND(COUNT(DISTINCT il.TrackId) * 100.0 / track_counts.TotalTracksByArtist, 2) AS PercentOfTracksSold,\n  -- Revenue per total track by artist\n  ROUND(SUM(il.UnitPrice * il.Quantity) / track_counts.TotalTracksByArtist, 2) AS RevenuePerTotalTrack\nFROM InvoiceLine il\nJOIN Track t ON il.TrackId = t.TrackId\nJOIN Album al ON t.AlbumId = al.AlbumId\nJOIN Artist ar ON ar.ArtistId = al.ArtistId\n-- Subquery to get total number of tracks in each genre\nJOIN (\n    SELECT \n      al.ArtistId,\n      COUNT(*) AS TotalTracksByArtist\n    FROM Track t\n    JOIN Album al ON t.AlbumId = al.AlbumId\n    GROUP BY al.ArtistId\n) AS track_counts ON ar.ArtistId = track_counts.ArtistId\nGROUP BY ar.Name, Year, track_counts.TotalTracksByArtist\n-- Arrange by TotalRevenue (Highest to Lowest)\nORDER BY Year, TotalRevenue DESC;\"\"\"\n  ).df()\n  \n# Combine data frames and ensure consistent types\nres_genre_df = pd.concat([\n  res_genre_agg_df,\n  res_genre_yearly_df.assign(Year=res_genre_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n  \nres_artist_df = pd.concat([\n  res_artist_agg_df,\n  res_artist_yearly_df.assign(Year=res_artist_yearly_df['Year'].astype(str))\n  ], ignore_index=True)\n\n# Get unique years\nyears = [\"All\"] + sorted(res_genre_df[res_genre_df['Year'] != 'All']['Year'].unique().tolist())\n\n# Get unique genres, sorted by total revenue share\ngenres = (\n    res_genre_df[res_genre_df['Year'] == 'All']\n    .sort_values(by='TotalRevenue', ascending=False)['Genre']\n    .tolist()\n)\n\n## Make Genre an ordered categorical (for plots)\nres_genre_df['Genre'] = pd.Categorical(\n  res_genre_df['Genre'], \n  categories=genres, \n  ordered=True\n  )\n\nres_genre_df = res_genre_df.sort_values(['Genre', 'Year'])\n\n# Get unique artists, sorted by total revenue share\nartists = (\n    res_artist_df[res_artist_df['Year'] == 'All']\n    .sort_values(by='TotalRevenue', ascending=False)['Artist']\n    .tolist()\n)\n\n## Make Artist an ordered categorical (for plots)\nres_artist_df['Artist'] = pd.Categorical(\n  res_artist_df['Artist'], \n  categories=artists, \n  ordered=True\n  )\n  \nres_artist_df = res_artist_df.sort_values(['Artist', 'Year'])\n\n\n\n\n\n\nRevenue by Genre: Stacked Bar Chart\nTo better understand genre-level commercial performance, revenue was analyzed both in total and normalized by catalog size. This approach distinguished high-volume genres from those that generated more revenue per available track. The results were visualized with stacked plots to compare total earnings and catalog-adjusted efficiency over time.\n\nRPython\n\n\n\n\nShow Code\n# TO DO: Figure out how to get the x-axis values to show\n\n# Stacked Area Plot: Total Revenue by Genre\np1 &lt;- ggplot2::ggplot(\n  res_genre_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year =  forcats::fct_rev(Year)), \n  ggplot2::aes(x = Genre, y = TotalRevenue, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Total Revenue\",\n    x = \"Genre\",\n    y = \"Total Revenue (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Turn X-axis text horizontal (legibility)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5, hjust = 1)\n    )\n\n# Stacked Area Plot: Revenue per Track in Catalog by Genre\np2 &lt;- ggplot2::ggplot(\n  res_genre_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year = forcats::fct_rev(Year)), \n  ggplot2::aes(x = Genre, y = RevenuePerTotalTrack, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Revenue per Track in Catalog\",\n    x = \"Genre\",\n    y = \"Revenue per Track (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Remove X-axis label (large text & in prev plot)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x=element_blank()\n    )\n\n## Convert to interactive plotly plots\np1_int &lt;- plotly::ggplotly(p1)\np2_int &lt;- plotly::ggplotly(p2)\n\n# For p2, turn off legend for all traces (already in p1)\nfor (i in seq_along(p2_int$x$data)) {\n  p2_int$x$data[[i]]$showlegend &lt;- FALSE\n}\n\n# Combine plots and display\nplotly::subplot(\n  p1_int, \n  p2_int, \n  nrows = 2, \n  shareX = TRUE, \n  titleY = TRUE\n) %&gt;%\n  plotly::layout(\n    title = list(text = \"Revenue by Genre&lt;br&gt;&lt;sup&gt;2009-01-01 to 2013-12-22&lt;/sup&gt;\"),\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.1)\n  )\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Filter out \"All\" year\ndf = res_genre_df[res_genre_df[\"Year\"] != \"All\"]\n\n# Sort years in reverse\nyears_r = sorted(df[\"Year\"].unique(), reverse=True)\n\n# Create subplot\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.08,\n    subplot_titles=(\"Total Revenue\", \"Revenue per Track in Catalog\")\n)\n\n# Colorblind-friendly color scheme\ncolors = px.colors.sequential.Viridis[:-len(years_r)]\n\n# Plot 1: Total Revenue\nfor i, year in enumerate(years_r):\n    subset = df[df[\"Year\"] == year]\n    fig.add_trace(\n        go.Bar(\n            x=subset[\"Genre\"],\n            y=subset[\"TotalRevenue\"],\n            name=str(year),\n            marker_color=colors[i],\n            showlegend=True if i == 0 else False  # only once for cleaner display\n        ),\n        row=1, col=1\n    )\n\n\n                        \n                                            \n\n\nShow Code\n# Plot 1: Revenue per Track in Catalog\nfor i, year in enumerate(years_r):\n    subset = df[df[\"Year\"] == year]\n    fig.add_trace(\n        go.Bar(\n            x=subset[\"Genre\"],\n            y=subset[\"RevenuePerTotalTrack\"],\n            name=str(year),\n            marker_color=colors[i],\n            showlegend=True  # show for all so full legend appears\n        ),\n        row=2, col=1\n    )\n\n\n                        \n                                            \n\n\nShow Code\n# Format Layout\nfig.update_layout(\n  ## Make stacked bar plot\n    barmode='stack',\n    title=dict(\n        text='Revenue by Genre&lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;',\n        x=0.5,\n        xanchor='center',\n        font=dict(size=20)\n    ),\n    height=700,\n    margin=dict(t=120),\n    ## Legend on bottom, horizontal\n    legend=dict(\n        orientation='h',\n        yanchor='bottom',\n        y=-0.25,\n        xanchor='center',\n        x=0.5,\n        title='Year'\n    )\n)\n\n\n                        \n                                            \n\n\nShow Code\n# Format axes\n## Y-axis as dollars\nfig.update_yaxes(title_text=\"Total Revenue (USD)\", row=1, col=1, tickformat=\"$,.2f\")\n\n\n                        \n                                            \n\n\nShow Code\nfig.update_yaxes(title_text=\"Revenue per Track (USD)\", row=2, col=1, tickformat=\"$,.2f\")\n\n\n                        \n                                            \n\n\nShow Code\n## Ensure only one x-axis shows, rotated (for legibility)\nfig.update_xaxes(title_text=None, row=1, col=1)\n\n\n                        \n                                            \n\n\nShow Code\nfig.update_xaxes(title_text=\"Genre\", tickangle=45, row=2, col=1)\n\n\n                        \n                                            \n\n\nShow Code\n# Display plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nGenre-level revenue remains stable over time, with no significant year-over-year shifts. This aligns with earlier findings: a small number of genres — primarily Rock, along with Latin, Metal, Alternative & Punk, and TV Shows — consistently drive the majority of revenue.\nRevenue per track in catalog tells a different story. While Rock shows consistent but moderate returns (likely due to its large catalog volume), several niche genres outperform in efficiency:\n\nSci Fi & Fantasy shows disproportionately high revenue per track, largely driven by premium TV content such as Battlestar Galactica. This genre saw a strong spike in 2011–2012 before tapering off in 2013 — suggesting a temporary surge in popularity.\nComedy experienced an anomalous peak in 2012, attributable to sales of The Office episodes.\nBossa Nova displayed a notable jump in 2012 revenue per catalog track, indicating strong demand relative to catalog size. Though less pronounced, this trend continued into 2013, hinting at sustained niche appeal.\nTV Show performance is likely undervalued in the current database structure, as Battlestar Galactica and The Office (TV Shows classified as Sci Fi & Frantasy and Comedy, respectively) are drivers of high per-track revenue in genres that are otherwise unrepresented in total revenue.\n\n\nOpportunities:\n\nInvest in high-margin, premium content.\nTV-based genres such as Sci Fi & Fantasy and Comedy show strong per-track performance despite limited catalog sizes. Expanding these offerings — particularly episodic or narrative-driven media — could unlock outsized returns relative to content volume.\nCapitalize on short-term demand spikes.\nGenres like Bossa Nova and Comedy demonstrated temporal revenue surges. Identifying and acting on these trends through curated promotions, seasonal playlists, or limited-time bundles may help convert interest into sustained revenue.\nReevaluate catalog strategies for large-volume genres.\nWhile Rock contributes the most revenue overall, its performance per track is relatively modest. Optimizing underperforming catalog segments — through targeted marketing or re-packaging — may improve monetization efficiency without requiring new content.\nMonitor genre saturation and track penetrance.\nGenres with high sales penetration (e.g., Sci Fi & Fantasy) may signal either strong consumer interest or an exhausted catalog. Ensuring a consistent stream of new content in high-penetration genres can help maintain engagement and avoid plateaus.\n\n\n\n\n\nArtist Revenue\n\n\nRevenue by Artist: Stacked Bar Chart\nTo better understand artist-level commercial performance, revenue was analyzed both in total and normalized by catalog size. This approach distinguished high-volume artists from those that generated more revenue per available track. The results were visualized with stacked plots to compare total earnings and catalog-adjusted efficiency over time.\n\nRPython\n\n\n\n\nShow Code\n# TO DO: Figure out how to make the x-axis values show\n\n# Stacked Area Plot: Total Revenue by Artist\np1 &lt;- ggplot2::ggplot(\n  res_artist_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year =  forcats::fct_rev(Year)) |&gt;\n   ## Restrict to top 20 artists for legibility\n    dplyr::filter(Artist %in% artists[1:20]) |&gt;\n    dplyr::mutate(Artist = factor(Artist, levels = artists[1:20], ordered = T)), \n  ggplot2::aes(x = Artist, y = TotalRevenue, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Total Revenue\",\n    x = \"Artist\",\n    y = \"Total Revenue (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Turn X-axis text horizontal (legibility)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5, hjust = 1)\n    )\n\n# Stacked Area Plot: Revenue per Track in Catalog by Artist\np2 &lt;- ggplot2::ggplot(\n  res_artist_df |&gt;\n    dplyr::filter(Year != \"All\") |&gt;\n    dplyr::mutate(Year = forcats::fct_rev(Year)) |&gt;\n   ## Restrict to top 20 artists for legibility\n    dplyr::filter(Artist %in% artists[1:20]) |&gt;\n    dplyr::mutate(Artist = factor(Artist, levels = artists[1:20], ordered = T)), \n  ggplot2::aes(x = Artist, y = RevenuePerTotalTrack, fill = Year)\n  ) +\n  ggplot2::geom_bar(stat=\"identity\") +\n  # Format Y-xis into dollars\n  ggplot2::scale_y_continuous(labels = scales::dollar) +\n  # Labels\n  ggplot2::labs(\n    title = \"Revenue per Track in Catalog\",\n    x = \"Artist\",\n    y = \"Revenue per Track (USD$)\",\n    fill = \"Year\"\n    ) +\n  # Colorblind friendly color scheme\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::theme_minimal() +\n  # Make Legend Vertical, Remove X-axis label (large text & in prev plot)\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.direction = \"vertical\",\n    axis.text.x=element_blank()\n    )\n\n## Convert to interactive plotly plots\np1_int &lt;- plotly::ggplotly(p1)\np2_int &lt;- plotly::ggplotly(p2)\n\n# For p2, turn off legend for all traces (already in p1)\nfor (i in seq_along(p2_int$x$data)) {\n  p2_int$x$data[[i]]$showlegend &lt;- FALSE\n}\n\n# Combine plots and display\nplotly::subplot(\n  p1_int, \n  p2_int, \n  nrows = 2, \n  shareX = TRUE, \n  titleY = TRUE\n) %&gt;%\n  plotly::layout(\n    title = list(text = \"Revenue by Artist&lt;br&gt;&lt;sup&gt;2009-01-01 to 2013-12-22&lt;/sup&gt;\"),\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.1)\n  )\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Filter out \"All\" year\ndf = res_artist_df[res_artist_df[\"Year\"] != \"All\"]\n# Restrict to top 20 artists (for legibility)\ndf = df[df[\"Artist\"].isin(artists[0:20])]\n\n# Sort years in reverse\nyears_r = sorted(df[\"Year\"].unique(), reverse=True)\n\n# Create subplot\nfig = make_subplots(\n    rows=2, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.08,\n    subplot_titles=(\"Total Revenue\", \"Revenue per Track in Catalog\")\n)\n\n# Colorblind-friendly color scheme\ncolors = px.colors.sequential.Viridis[:-len(years_r)]\n\n# Plot 1: Total Revenue\nfor i, year in enumerate(years_r):\n    subset = df[df[\"Year\"] == year]\n    fig.add_trace(\n        go.Bar(\n            x=subset[\"Artist\"],\n            y=subset[\"TotalRevenue\"],\n            name=str(year),\n            marker_color=colors[i],\n            showlegend=True if i == 0 else False  # only once for cleaner display\n        ),\n        row=1, col=1\n    )\n\n\n                        \n                                            \n\n\nShow Code\n# Plot 1: Revenue per Track in Catalog\nfor i, year in enumerate(years_r):\n    subset = df[df[\"Year\"] == year]\n    fig.add_trace(\n        go.Bar(\n            x=subset[\"Artist\"],\n            y=subset[\"RevenuePerTotalTrack\"],\n            name=str(year),\n            marker_color=colors[i],\n            showlegend=True  # show for all so full legend appears\n        ),\n        row=2, col=1\n    )\n\n\n                        \n                                            \n\n\nShow Code\n# Format Layout\nfig.update_layout(\n  ## Make stacked bar plot\n    barmode='stack',\n    title=dict(\n        text='Revenue by Artist&lt;br&gt;&lt;sub&gt;2009-01-01 to 2013-12-22&lt;/sub&gt;',\n        x=0.5,\n        xanchor='center',\n        font=dict(size=20)\n    ),\n    height=700,\n    margin=dict(t=120),\n    ## Legend on bottom, horizontal\n    legend=dict(\n        orientation='h',\n        yanchor='bottom',\n        y=-0.25,\n        xanchor='center',\n        x=0.5,\n        title='Year'\n    )\n)\n\n\n                        \n                                            \n\n\nShow Code\n# Format axes\n## Y-axis as dollars\nfig.update_yaxes(title_text=\"Total Revenue (USD)\", row=1, col=1, tickformat=\"$,.2f\")\n\n\n                        \n                                            \n\n\nShow Code\nfig.update_yaxes(title_text=\"Revenue per Track (USD)\", row=2, col=1, tickformat=\"$,.2f\")\n\n\n                        \n                                            \n\n\nShow Code\n## Ensure only one x-axis shows, rotated (for legibility)\nfig.update_xaxes(title_text=None, row=1, col=1)\n\n\n                        \n                                            \n\n\nShow Code\nfig.update_xaxes(title_text=\"Artist\", tickangle=45, row=2, col=1)\n\n\n                        \n                                            \n\n\nShow Code\n# Display plot\nfig.show()\n\n\n                        \n                                            \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nArtist revenue is temporally consistent.\nMost artists show stable annual revenue across the 2009–2013 period. While individual years may fluctuate (e.g., dips for Metallica and Led Zeppelin in 2011), these appear to be temporary rather than indicative of long-term trends.\nAverage revenue per track sold is remarkably stable (~$0.75).\nA small number of high-volume artists (e.g., Deep Purple, Pearl Jam, Van Halen) show lower per-track revenue (~$0.50), suggesting that large catalogs may dilute revenue unless paired with sustained demand. This could indicate catalog saturation or uneven consumer engagement across their offerings.\nRevenue per track in catalog highlights structural differences.\nThis metric remains consistent for most music artists, but TV show content (e.g., Lost, The Office) shows a different pattern: exceptionally high revenue per catalog track but concentrated in specific years. For instance, Lost peaked in 2012 before falling to zero in 2013, and The Office spiked from 2010–2012 with no revenue in adjacent years.\nTV Shows demonstrate high efficiency per asset.\nDespite small catalogs, TV content generates significantly higher revenue per track than traditional music. This suggests strong demand relative to supply — possibly pointing to unmet or episodic consumption patterns.\n\nOpportunities:\n\nExpand premium TV show content.\nThe high return per catalog item in TV genres suggests an underexploited category. Adding similar content (episodic, narrative-driven media) could yield high revenue efficiency without requiring large-scale production.\nReinvigorate large but underperforming catalogs.\nArtists with expansive libraries but low per-track revenue (e.g., Deep Purple, Pearl Jam) may benefit from targeted marketing efforts such as curated collections, remastered releases, or highlight reels that surface their most engaging content.\nUse consumption patterns to guide licensing and promotions.\nThe temporal spikes in shows like Lost and The Office suggest windows of elevated demand. Monitor and act on these seasonal or cultural surges to time promotions, exclusive bundles, or re-releases effectively.\nMonitor catalog saturation signals.\nWhen large catalogs yield diminishing per-track returns, it may be time to reassess content strategy. This includes retiring underperforming assets or introducing new content types that align better with current listener interests."
  },
  {
    "objectID": "projects/draft.html#q2-summary",
    "href": "projects/draft.html#q2-summary",
    "title": "SQL-Driven Business Intelligence Dashboard (Chinook ‘Digital Music Store’)",
    "section": "Summary of Exploratory Findings",
    "text": "Summary of Exploratory Findings\nWhile overall genre revenue remains stable across years, genre-specific performance varies widely depending on how it’s measured:\n\nTotal Revenue reflects market size. Rock dominates here, alongside Latin, Metal, and Alternative.\nRevenue per Track in Catalog reveals efficiency. Sci Fi & Fantasy, Comedy, and Bossa Nova perform disproportionately well relative to their catalog sizes — often driven by episodic TV content.\n\nThese patterns suggest that catalog volume and monetization efficiency are often misaligned. Notably, the top-performing genres by revenue-per-track tend to be narrow in scope but high in engagement, hinting at an opportunity to rebalance the content portfolio.\nOn the artist level, similar trends likely exist (see artist data prep), and deeper analysis could surface key revenue drivers hiding behind genre-level aggregates.\n\nMost Efficient Genres by Revenue per Track (2012 spike): Sci Fi & Fantasy (Battlestar Galactica), Comedy (The Office), Bossa Nova\nTop Genres by Total Revenue: Rock, Latin, Metal, Alternative & Punk, TV Shows\n\n\n\n\n\n\n\nPossible Next Steps\n\n\n\nDeepen the Analysis:\n\nAttribute-level analysis for top-performing genres.\nDisaggregate genre revenue by artist, album, or track to pinpoint the specific drivers of efficiency within high-performing niches.\nExplore customer-level purchase behavior.\nAre niche genre buyers also more valuable customers overall? Understanding cross-genre engagement can inform bundling or recommendation strategies.\n\nStrategic Business Opportunities:\n\nInvest in premium, episodic content.\nSci Fi & Fantasy and Comedy genres — driven by TV content — deliver strong returns per track. Expanding this segment could yield high-margin growth.\nReact quickly to demand spikes.\nTemporal surges in Bossa Nova and Comedy suggest that curated playlists or limited-time offers can help capitalize on emerging trends.\nReoptimize large, underperforming genres.\nRock has broad catalog coverage but moderate per-track performance. Marketing, repackaging, or removal of underperformers could improve efficiency.\nPrioritize catalog expansion where penetration is high.\nFor genres with high track sales penetration (e.g., Sci Fi), maintaining momentum may require fresh content to meet continuing demand."
  }
]